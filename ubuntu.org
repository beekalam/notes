#+TITLE:  ubuntu 
#+Languge: en
#+STARTUP: overview
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/main.css" />
#+HTML_HEAD: <script src="js/ganalytics.js" async></script>
#+AUTHOR:  Mohammad Reza Mansouri
#+STARTUP: overview

* network
** config
Ubuntu and all the Debian-based distributions use a unique file to
configure all the network interfaces; this file is named */etc/network/interfaces*.

An Ethernet interface is configured in either the DHCP or static mode. If it is the
DHCP mode, we will find the following lines in */etc/network/interfaces* :

#+begin_src 
auto eth0
iface eth0 inet dhcp
#+end_src

Here, the line containing the auto keyword means that this interface should be
automatically brought up when a computer boots up.

If it is in the static mode, you will find the output that looks like the following lines:

#+begin_src 
iface eth0 inet static
address 192.168.1.58
netmask 255.255.255.0
gateway 192.168.1.1
#+end_src 

** =resolve.conf= DNS config
=/etc/resolv.conf= is another network configuration file which
contains the DNS list used by the server. If you are using DHCP, the
content of this file will be set automatically. You can edit it by
adding your own/favorite DNS servers in the following format:

#+begin_src 
nameserver 192.168.1.11
nameserver 192.168.1.12
#+end_src

starting from Ubuntu 12.04 and later, =/etc/resolve.conf= is located at:

#+begin_src 
sudo vi /etc/resolvconf/resolv.conf.d/base
#+end_src

** put wireless card in monitor mode
   
#+begin_src shell
sudo ifconfig wlan1 down
sudo iwconfig wlan1 mode monitor
# Check:
iwconfig
#+end_src
 
** add bridges to tor
1) https://bridges.torproject.org/
2) add the bridges at the end of  torrc

 #+begin_src
 UseBridges 1
 Bridge <ip>:<port> <fingerprint>
 #+end_src
 
** privoxy
*** use privoxy to http over socks

 open =~/etc/privoxy/config/= and comment out the line

 #+begin_src 
 #        forward-socks5t             /     127.0.0.1:9050 .
 #+end_src 

 and maybe

 #+begin_src 
 #        forward         192.168.*.*/     .
 #        forward            10.*.*.*/     .
 #        forward           127.*.*.*/     .
 #+end_src 

 and

 #+begin_src 
 #        forward           localhost/     .
 #+end_src 

*** add exception to privoxy forward
#+begin_src 
forward .somesite.com  .
#+end_src 

** network open sockets by program name?

#+begin_src 
$ netstat -ltpu
#+end_src 

** bandwidth usage by program name?
   
#+begin_src shell
$ sudo nethogs <NIC>
#+end_src 

** static route
   
To set up a static route on Ubuntu so that it automatically gets
created every time the PC starts, you need to modify the
=/etc/network/interfaces= file.

#+begin_src 
$ sudo nano /etc/network/interfaces
#+end_src
 
The route will get set up when one of the interfaces comes up. Find
the section that corresponds to the interface that this route will be
set up on. Eg. eth0 or em1.

At the bottom of this section, add the following line:

#+begin_src 
up route add -net 192.168.40.0/24 gw 192.168.30.1 dev em1
up route add -net 172.16.11.0/24 gw 172.16.12.141 dev eth0:0
#+end_src
 
** nmcli

#+begin_src shell
$ nmcli device wifi list    # the same as sudo iw wlp12s0 scan | grep SSID
# use nmcli to display configured permissions with the following command:
$ nmcli general permissions

#creating  a connection
$ sudo nmcli connection add con-name wired-home \
ifname enp9s0 type ethernet ip4 192.168.0.8 gw4 192.168.0.1
Connection 'wired-home' (e17cb6b7-685f-4cf2-9e8b-16cbfae1f73a)
successfully added.
#add the DNS configuration to the connection profile
$ sudo nmcli connection modify wired-home ipv4.dns "192.168.0.3 8.8.8.8"
# We can now display the properties with the following command:
$ nmcli -p connection show wired-home
#+end_src 

** SAMBA
*** Listing available smb shares on a network through the command line in linux
#+begin_src shell 
#This command is a very little known secret of Samba. It returns IP adresses of
#all Samba servers in one's own broadcast domain:

$ nmblookup __SAMBA__

#This one returns a list of all NetBIOS names and their aliases of all Samba
#servers in the neighbourhood (it does a 'node status query'):

$ nmblookup -S __SAMBA__

#This one returns a list of all IP adresses of SMB servers (that is,
#Linux+Unix/Samba or Windows) in the neighbourhood:

$ nmblookup '*'

#Finally, all NetBIOS names and their aliases of all SMB servers (Linux+Unix/Samba or Windows):

$ nmblookup -S '*'
#+end_src

* ssh
** installation

#+begin_src shell
$ sudo apt-get install openssh-client
$ sudo apt-get install openssh-server
$ sudo apt-get install ssh   # installes both the client and the server
#+end_src

** ssh escape sequences
all you’ve to do is press [Enter] key followed by ~.. Please note that
escapes are only recognized immediately after newline.

| command | description                                                                                              |
|---------+----------------------------------------------------------------------------------------------------------|
| ~.      | Disconnect.                                                                                              |
|         |                                                                                                          |
| ~^Z     | Background ssh.                                                                                          |
|         |                                                                                                          |
| ~#      | List forwarded connections.                                                                              |
|         |                                                                                                          |
| ~&      | Background ssh at logout when waiting for forwarded connection / X11 sessions to terminate.              |
|         |                                                                                                          |
| ~?      | Display a list of escape characters.                                                                     |
|         |                                                                                                          |
| ~B      | Send a BREAK to the remote system (only useful for SSH protocol version 2 and if the peer supports it).  |
|         |                                                                                                          |
| ~C      | Open command line.                                                                                       |
|         |                                                                                                          |
| ~R      | Request rekeying of the connection (only useful for SSH protocol version 2 and if the peer supports it). |

** use a different key to connect 
#+begin_src shell
$ ssh -i key_file root@host
#+end_src 

* vmware
** start virtual machine in background
   
#+begin_src 
vmrun -T ws start /export/vmware/rh5/server.vmx nogui
#+end_src
 
* Misc
** remap caps lock to ctrl

To permanently change the behaviour:
#+begin_src 
    run dconf-editor

    select org.gnome.desktop.input-sources

    Change xkb-options to ['ctrl:nocaps'] (or add it to any existing options)
#+end_src 

or on the command line (Warning -- this overwrites your existing settings!):
#+begin_src 
gsettings set org.gnome.desktop.input-sources xkb-options "['ctrl:nocaps']"
#+end_src 

** how to tell which version of library you have?
#+begin_src shell
$ dpkg -l '*ssh*'
#+end_src 

** time
*** sync time 
#+begin_src shell
$ sudo date -s "$(wget -qSO- --max-redirect=0 google.com 2>&1 | grep Date: | cut -d' ' -f5-8)Z"
#+end_src 

*** change CPU affinity for a process
#+begin_src shell
$ taskset -pc 0 `pidof recoll`
#+end_src 

** recoll
#+begin_src 
kchmviewer --url %i %F
#+end_src 

** display current date and time
#+begin_src shell
$ cat /etc/timezone
$ grep UTC /etc/default/rcS
$ date
# hardware clock
$ sudo hwclock --show
$ timedatectl
#+end_src

** mercurial download big repository
#+begin_src shell
$ hg clone --rev 100 <remote URL> <local path>
$ cd <local path>
$ hg pull --rev 200
$ hg pull --rev 300
# use pull update in order to get the pullied tip into your working directory
$ hg  update
#+end_src

* groups
*** Add a New Group

To add a new group, all you need to do is use the groupadd command like so:
#+begin_src
groupadd <groupname>
#+end_src 

*** Add an Existing User to a Group
    
Next we’ll add a user to the group, using this syntax:
#+begin_src shell
$ usermod -a -G <groupname> username
#+end_src 

For example, to add user geek to the group admins, use the following command:
#+begin_src 
$ usermod -a -G admins geek
#+end_src 

*** Change a User’s Primary Group

Sometimes you might want to switch out the primary group that a user is assigned to, which you can do with this command:
#+begin_src 
usermod -g <groupname> username
#+end_src 

*** View a User’s Group Assignments

If you’re trying to figure out a permissions issue, you’ll want to use the id command to see what groups the user is assigned to:
#+begin_src shell
id <username>

# This will display output something like this:

uid=500(howtogeek) gid=500(howtogeek) groups=500(howtogeek), 1093(admins)
#+end_src 

You can also use the groups command if you prefer, though it is the same as using id -Gn <username>.
#+begin_src shell
$ groups <username>
#+end_src 

*** View a List of All Groups

To view all the groups on the system, you can just use the groups command:
#+begin_src shell
$ groups
#+end_src 

Add a New User and Assign a Group in One Command

Sometimes you might need to add a new user that has access to a particular
resource or directory, like adding a new FTP user. You can do so with the
useradd command:
#+begin_src shell 
$ useradd -g <groupname> username
#+end_src 

For instance, lets say you wanted to add a new user named jsmith to the ftp group:
#+begin_src shell 
$ useradd -G ftp jsmith
#+end_src 

And then you’ll want to assign a password for that user, of course:
#+begin_src shell 
$ passwd jsmith
#+end_src 

Add a User to Multiple Groups

You can easily add a user to more than one group by simply specifying them in a
comma-delimited list, as long as you are assigning the secondary groups:
#+begin_src shell 
$ usermod -a -G ftp,admins,othergroup <username>
#+end_src 

That should cover everything you need to know about adding users to groups on Linux.

* file
** convert cue disk image to iso format?
Typically a .cue file will be accompanied by a .bin file that contains
the actual image data.  If you'd like to convert it to the .iso
format, the Iso9660 Analyzer Tool (-get install iat) should do the
trick:

#+begin_src shell
$ iat my_image.bin my_new_image.iso
#+end_src

** show recently modified/created files?

#+begin_src shell
$ find ${1} -type f | xargs stat --format '%Y :%y %n' 2>/dev/null | sort -nr | cut -d: -f2-
#+end_src

** empty a log file

#+begin_src shell
$ cat /dev/null > logfile
$ cp /dev/null largefile.txt
$ dd if=/dev/null of=logfile    # shows how long it takes
$ truncate logfile --size 0
#+end_src 

** searching
*** Find
** find by name
#+begin_src shell 
$ find -name "query"

# To find a file by name, but ignore the case of the query, type:
$ find -iname "query"

# If you want to find all files that don't adhere to a specific pattern, you
# can invert the search with "-not" or "!". If you use "!", you must escape
# the character so that bash does not try to interpret it before find can act:
$ find -not -name "query_to_avoid"
#Or
$ find \! -name "query_to_avoid"
#+end_src

** by type
You can specify the type of files you want to find with the "-type" parameter. It works like this:

find -type type_descriptor query
Some of the most common descriptors that you can use to specify the type of file are here:

f: regular file

d: directory

l: symbolic link

c: character devices

b: block devices

For instance, if we wanted to find all of the character devices on our system, we could issue this command:
#+begin_src shell 
$ find / -type c

# We can search for all files that end in ".conf" like this:
$ find / -type f -name "*.conf"
#+end_src 

** Filtering by Size
We add a suffix on the end of our value that specifies how we are counting. These are some popular options:

c: bytes

k: Kilobytes

M: Megabytes

G: Gigabytes

b: 512-byte blocks

#+begin_src shell 
# To find all files that are exactly 50 bytes, type:
$ find / -size 50c

# To find all files less than 50 bytes, we can use this form instead:
$ find / -size -50c

# To Find all files more than 700 Megabytes, we can use this command:
$ find / -size +700M
#+end_src

** Filter by Time
inux stores time data about access times, modification times, and change times.

Access Time: Last time a file was read or written to.

Modification Time: Last time the contents of the file were modified.

Change Time: Last time the file's inode meta-data was changed.

We can use these with the "-atime", "-mtime", and "-ctime" parameters. These can
use the plus and minus symbols to specify greater than or less than, like we did
with size.
#+begin_src shell 
#To find files that have a modification time of a day ago, type:
$ find / -mtime 1

# If we want files that were accessed in less than a day ago, we can type:
$ find / -atime -1

#To get files that last had their meta information changed more than 3 days ago, type:
$ find / -ctime +3

# There are also some companion parameters we can use to specify minutes instead of days:
$ find / -mmin -1

#This will give the files that have been modified type the system in the last minute.
Find can also do comparisons against a reference file and return those that are newer:
$ find / -newer myfile
#+end_src


Some important options:
-x (on BSD) -xdev (on Linux)       Stay on the same file system (dev in fstab).
-exec cmd {} \;       Execute the command and replace {} with the full path
-iname       Like -name but is case insensitive
-ls       Display information about the file (like ls -la)
-size n       n is +-n (k M G T P)
-cmin n       File's status was last changed n minutes ago.

#+begin_src shell

$ find . -type f ! -perm -444        # Find files not readable by all
$ find . -type d ! -perm -111        # Find dirs not accessible by all
$ find /home/user/ -cmin 10 -print   # Files created or modified in the last 10 min.
$ find . -name '*.[ch]' | xargs grep -E 'expr' # Search 'expr' in this dir and below.
$ find / -name "*.core" | xargs rm   # Find core dumps and delete them (also try core.*)
$ find / -name "*.core" -print -exec rm {} \;  # Other syntax
$ Find images and create an archive, iname is not case sensitive. -r for append
$ find . \( -iname "*.png" -o -iname "*.jpg" \) -print -exec tar -rf images.tar {} \;
$ find . -type f -name "*.txt" ! -name README.txt -print  # Exclude README.txt files
$ find /var/ -size +10M -exec ls -lh {} \;     # Find large files > 10 MB
$ find /var/ -size +10M -ls           # This is simpler
$ find . -size +10M -size -50M -print
$ find /usr/ports/ -name work -type d -print -exec rm -rf {} \;  # Clean the ports
$ Find files with SUID; those file are vulnerable and must be kept secure
$ find / -type f -user root -perm -4000 -exec ls -l {} \;
$ find /home/ -name "*~"   #find tilde files (backup files)
# Find all the files directly under the /etc/ directory that start with the letter p
# and end in anything using the following command:
$ find / -regex '^/etc/p[a-z]*$'

# Find all the files on the filesystem that are called configuration, ignoring case,
# and accommodating abbreviations such as confg , cnfg , and cnfig using the
# following command:
$ find / -regex '^[/a-z_]*[cC]+[Oo]*[nN]+[fF]+[iI]*[gF]+$'

# This command will find and delete anything reachable one level from the
# root that has a name such as 'virus'—case-insensitive.

$ find / -regex '^/[a-z_\-]*/[Vv][iI][rR][uS]*$' –delete

$ find /etc/ -maxdepth 1 -name passwd -exec stat {} \;

# look for files larger than specified number(1M)
$ find ~ -type f -name "*.JPG" -size +1M

# We would look for all the files with permissions that are not 0600 
# and the directories with permissions that are not 0700.
$ find ~ \( -type f -not -perm 0600 \) -or \( -type d -not -perm 0700 \)

# delete files that have file extension ".BAK"
$ find ~ -type f -name '*.BAK' -delete

# execute user defined action interactively using -ok
$ find ~ -type f -name 'foo*' -ok ls -l '{}' ';'

# dealing with filenames with spaces
# -print0 provides null-separated output
# xargs has --null option, accepts null separated input.
# A null character is defined in ASCII as the character repre-sented by the number zero
$ find ~ -iname '*.jpg' -print0 | xargs --null ls -l

#+end_src

*** locate 
#+begin_src shell
# will search its database of pathnames and output any that contain the string "bin/zip"
$ locate bin/zip
#+end_src

** Filter by owner and permission
You do this by using the "-user" and "-group" parameters respectively. 
#+begin_src shell 
# Find a file that is owned by the "syslog" user by entering:
$ find / -user syslog

# Similarly, we can specify files owned by the "shadow" group by typing:
$ find / -group shadow

#We can also search for files with specific permissions.
#If we want to match an exact set of permissions, we use this form:
$ find / -perm 644

#This will match files with exactly the permissions specified.

#If we want to specify anything with at least those permissions, you can use this form:
$ find / -perm -644
#+end_src


* php

if php does not get executed.
#+begin_src shell
sudo apt-get install libapache2-mod-php7.0
#+end_src
 
* wget
** ignore robots.txt 

~-e robots=off~

** get the size of file before downloading
#+begin_src shell 
$ wget --spider <link>
$ curl --head <link>
#+end_src 

** wget download with proxy

Via =~/.wgetrc= file:

#+begin_src 
use_proxy=yes
http_proxy=127.0.0.1:8080
#+end_src 

or via -e options placed after the URL:

#+begin_src shell
$ wget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080 ...
#+end_src
 
*https proxy*
note you also need to set *https_proxy* if url is HTTPS

*with authentication*

http_proxy=http://username:password@proxy_host:proxy_port

http://stackoverflow.com/questions/11211705/setting-proxy-in-wget

* System
** Running kernel and system information:

#+begin_src shell
$ uname -a                                  # Get the kernel version (and BSD version)
$ lsb_release -a                         $ Full release info of any LSB distribution
$ cat /etc/debian_version         # Get Debian version
Use /etc/DISTR-release with DISTR= lsb (Ubuntu) /etc/issue.
$ uptime                                      # Show how long the system has been running + load
$ hostname                                # system's host name
$ hostname -i                            # Display the IP address of the host.
$ man hier                                 # Description of the file system hierarchy
$ last reboot                              # Show system reboot history
#+end_src

** Hardware Informations:
*Kernel detected hardware:*

#+begin_src shell 
$+begin_src shell
$ dmesg                               # Detected hardware and boot messages
$ lsdev                                  # information about installed hardware
$ dd if=/dev/mem bs=1k skip=768 count=256 2>/dev/null | strings -n 8 # Read BIOS

$ cat /proc/cpuinfo                               # CPU model
$ cat /proc/meminfo                             # Hardware memory
$ grep MemTotal /proc/meminfo       # Display the physical memory
$ watch -n1 'cat /proc/interrupts'        # Watch changeable interrupts continuously
$ free -m                                                # Used and free memory (-m for MB)
$ cat /proc/devices                              # Configured devices
$ lspci -tv                       # Show PCI devices
$ lsusb -tv                      # Show USB devices
$ lshal                            # Show a list of all devices with their properties
$ dmidecode                # Show DMI/SMBIOS: hw info from the BIOS

#+end_src
 
** Load, statistics and messages:
   
The following commands are useful to find out what is going on on the
system.

#+begin_src shell

$ top                                                   # display and update the top cpu processes
$ mpstat 1                                         # display processors related statistics
$ vmstat 2                                         # display virtual memory statistics
$ iostat 2                                           # display I/O statistics (2 s intervals)
$ systat -vmstat 1                            # BSD summary of system statistics (1 s intervals)
$ systat -tcp 1                                  # BSD tcp connections (try also -ip)
$ systat -netstat 1                           # BSD active network connections
$ systat -ifstat 1                               # BSD network traffic through active interfaces
$ systat -iostat 1                              # BSD CPU and and disk throughput
$ tail -n 500 /var/log/messages    # Last 500 kernel/syslog messages
$ tail /var/log/warn                          # System warnings messages see syslog.conf

#+end_src
 
*** Users

 #+begin_src shell

 # id                                                                     # Show the active user id with login and group
 # last                                                                  # Show last logins on the system
 # who                                                                 # Show who is logged on the system
 # groupadd admin                                           # Add group "admin" and user colin
 # useradd -c "Colin Barschel" -g admin -m colin
 # usermod -a -G                                               # Add existing user to group (Debian)
 # userdel colin                                                  # Delete user colin 
 # pw groupmod admin -m newmembe r      # Add a new member to a group
 # pw useradd colin -c "Colin Barschel" -g admin -m -s /bin/tcsh
 # pw userdel colin; pw groupdel admin
 #+end_src

*** Kernel modules

#+begin_src shell
$ lsmod                                      # List all modules loaded in the kernel
$ modprobe isdn                      # To load a module (here isdn)
#+end_src
 
*** Compile Kernel

#+begin_src shell
$ cd /usr/src/linux
$ make mrproper                      # Clean everything, including config files
$ make oldconfig                      # Reuse the old .config if existent
$ make menuconfig                 # or xconfig (Qt) or gconfig (GTK)
$ make                                       # Create a compressed kernel image
$ make modules                      # Compile the modules
$ make modules_install         # Install the modules
$ make install                           # Install the kernel
$ reboot
#+end_src

** processes

listing and pids

each process has a unique number, the pid. a list of all running process is
retrieved with ps.

#+begin_src shell 
# ps -auxefw                         # extensive list of all running process
#+end_src 

however more typical usage is with a pipe or with pgrep:

#+begin_src shell
$ ps axww | grep cron
586  ??  is     0:01.48 /usr/sbin/cron -s
$ ps axjf                                     # all processes in a tree format
$ ps aux | grep 'ss[h]'               # find all ssh pids without the grep pid
$ pgrep -l sshd                         # find the pids of processes by (part of) name
$ echo $$                                  # the pid of your shell
$ fuser -va 22/tcp                     # list processes using port 22 (linux)
$ pmap pid                               # memory map of process (hunt memory leaks) (linux)
$ fuser -va /home                     # list processes accessing the /home partition
$ strace df                                  # trace system calls and signals
$ truss df                                    # same as above
#+end_src 

** Signals/Kill

Terminate or send a signal with kill or killall.

#+begin_src shell
$ kill -s TERM 4712                  # same as kill -15 4712
$ killall -1 httpd                          # Kill HUP processes by exact name
$ pkill -9 http                              # Kill TERM processes by (part of) name
$ pkill -TERM -u www              # Kill TERM processes owned by www
$ fuser -k -TERM -m /home     # Kill every process accessing /home (to umount)
#+end_src 

Important signals are:

#+begin_src 
1       HUP (hang up)
2       INT (interrupt)
3       QUIT (quit)
9       KILL (non-catchable, non-ignorable kill)
15     TERM (software termination signal)
#+end_src 

** Permissions

Change permission and ownership with chmod and chown.  The default
umask can be changed for all users in /etc/profile for Linux.  The
default umask is usually 022. The umask is subtracted from 777, thus
umask 022 results in a permission 0f 755.

1 --x execute                        # Mode 764 = exec/read/write | read/write | read
2 -w- write                          # For:       |--  Owner  --|   |- Group-|   |Oth|
4 r-- read
ugo=a                              u=user, g=group, o=others, a=everyone

#+begin_src shell 
$ chmod [OPTION] MODE[,MODE] FILE    # MODE is of the form [ugoa]*([-+=]([rwxXst]))
$ chmod 640 /var/log/maillog                      # Restrict the log -rw-r-----
$ chmod u=rw,g=r,o= /var/log/maillog       # Same as above
$ chmod -R o-r /home/*                                # Recursive remove other readable for all users
$ chmod u+s /path/to/prog                           # Set SUID bit on executable (know what you do!)
$ find / -perm -u+s -print                               # Find all programs with the SUID bit
$ chown user:group /path/to/file                  # Change the user and group ownership of a file
$ chgrp group /path/to/file                             # Change the group ownership of a file
$ chmod 640 `find ./ -type f -print`                # Change permissions to 640 for all files
$ chmod 751 `find ./ -type d -print`               # Change permissions to 751 for all directories
#+end_src 

Disk information:
#+begin_src shell 
$ hdparm -I /dev/sda                 # information about the IDE/ATA disk (Linux)
$ fdisk /dev/ad2                          # Display and manipulate the partition table
$ smartctl -a /dev/ad2                # Display the disk SMART info
#+end_src

System mount points/Disk usage
#+begin_src shell 
$ mount | column -t                   # Show mounted file-systems on the system
$ df                                              # display free disk space and mounted devices
$ cat /proc/partitions                # Show all registered partitions
$ du -sh *                                 # Directory sizes as listing
$ du -csh                                 # Total directory size of the current directory
$ du -ks * | sort -n -r              # Sort everything by size in kilobytes
#+end_src 

Who has which files opened:
This is useful to find out which file is blocking a partition which has to be unmounted and gives a typical error of:

# umount /home/
umount: unmount of /home             # umount impossible because a file is locking home
   failed: Device busy
# ls -lSr                                               # Show files, biggest last

Find opened files on a mount point with fuser or lsof:

# fuser -m /home                     # List processes accessing /home
# lsof /home

COMMAND   PID    USER   FD   TYPE DEVICE    SIZE     NODE NAME
tcsh    29029 eedcoba  cwd    DIR   0,18   12288  1048587 /home/cipi (cipi:/home)
lsof    29140 eedcoba  cwd    DIR   0,18   12288  1048587 /home/cipi (cipi:/home)
About an application:

ps ax | grep Xorg | awk '{print $1}'
3324
# lsof -p 3324
COMMAND   PID    USER   FD   TYPE DEVICE    SIZE    NODE NAME
Xorg    3324 root    0w   REG        8,6   56296      12492 /var/log/Xorg.0.log
About a single file:
# lsof /var/log/Xorg.0.log
COMMAND  PID USER   FD   TYPE DEVICE  SIZE  NODE NAME
Xorg    3324 root    0w   REG    8,6 56296 12492 /var/log/Xorg.0.log

Mount/remount a file system

For example the cdrom. If listed in /etc/fstab:
#+begin_src 
# mount /cdrom
# mount -t auto /dev/cdrom /mnt/cdrom             # typical cdrom mount command
# mount /dev/hdc -t iso9660 -r /cdrom               # typical IDE
# mount /dev/scd0 -t iso9660 -r /cdrom             # typical SCSI cdrom
# mount /dev/sdc0 -t ntfs-3g /windows              # typical SCSI
#+end_src 

Entry in /etc/fstab:
#+begin_src 
/dev/cdrom   /media/cdrom  subfs noauto,fs=cdfss,ro,procuid,nosuid,nodev,exec 0 0
#+end_src 

Add swap on-the-fly
Suppose you need more swap (right now), say a 2GB file /swap2gb .


# dd if=/dev/zero of=/swap2gb bs=1024k count=2000
# mkswap /swap2gb                                            # create the swap area
# swapon /swap2gb                                             # activate the swap. It now in use
# swapoff /swap2gb                                             # when done deactivate the swap
# rm /swap2gb

Mount an SMB share

Suppose we want to access the SMB share myshare on the computer smbserver, the
address as typed on a Windows PC is \\smbserver\myshare\. We mount on
/mnt/smbshare. Warning> cifs wants an IP or DNS name, not a Windows name.

# smbclient -U user -I 192.168.16.229 -L //smbshare/        # List the shares
# mount -t smbfs -o username=winuser //smbserver/myshare /mnt/smbshare
# mount -t cifs -o username=winuser,password=winpwd //192.168.16.229/myshare /mnt/share

Additionally with the package mount.cifs it is possible to store the credentials in a file, for example /home/user/.smb:

username=winuser
password=winpwd
And mount as follow:
# mount -t cifs -o credentials=/home/user/.smb //192.168.16.229/myshare /mnt/smbshare

Mount an image:

# mount -t iso9660 -o loop file.iso /mnt                # Mount a CD image
# mount -t ext3 -o loop file.img /mnt                     # Mount an image with ext3 fs

** Create a memory file system

A memory based file system is very fast for heavy IO application. How
to create a 64 MB partition mounted on /memdisk:

#+begin_src shell
$ mount -t tmpfs -osize=64m tmpfs /memdisk
#+end_src 

** Disk performance

Read and write a 1 GB file on partition ad4s3c (/home)
#+begin_src shell
# time dd if=/dev/ad4s3c of=/dev/null bs=1024k count=1000
# time dd if=/dev/zero bs=1024k count=1000 of=/home/1Gb.file
# hdparm -tT /dev/hda      # Linux only
#+end_src 

** Networking

#+begin_src shell
# ethtool eth0                                           # Show the ethernet status (replaces mii-diag)
# ethtool -s eth0 speed 100 duplex full # Force 100Mbit Full duplex
# ethtool -s eth0 autoneg off # Disable auto negotiation
# ethtool -p eth1                                      # Blink the ethernet led - very useful when supported
# ip link show                                           # Display all interfaces on Linux (similar to ifconfig)
# ip link set eth0 up                                # Bring device up (or down). Same as "ifconfig eth0 up"
# ip addr show                                        # Display all IP addresses on Linux (similar to ifconfig)
# ip neighbor show                                      # Similar to arp -a
#+end_src 

** Ports in use
Listening open ports:
#+begin_src shell 
# netstat -an | grep LISTEN
# lsof -i                                         # List all Internet connections
# socklist                                     # Display list of open sockets
# netstat -anp --udp --tcp | grep LISTEN      
# netstat -tup                              # List active connections to/from system
# netstat -tupl                             # List listening ports from system
#+end_src 

** Firewall
Check if a firewall is running (typical configuration only):
#+begin_src shell 
# iptables -L -n -v                                 # For status Open the iptables firewall
# iptables -P INPUT       ACCEPT     # Open everything
# iptables -P FORWARD     ACCEPT
# iptables -P OUTPUT      ACCEPT
# iptables -Z                                         # Zero the packet and byte counters in all chains
# iptables -F                                         # Flush all chains
# iptables -X                                         # Delete all chains
#+end_src 

** IP Forward for routing

Check and then enable IP forward with :

#+begin_src shell 
# cat /proc/sys/net/ipv4/ip_forward  # Check IP forward 0=off, 1=on
# echo 1 > /proc/sys/net/ipv4/ip_forward
#+end_src 

or edit =/etc/sysctl.conf= with:

~net.ipv4.ip_forward = 1~

Network Address Translation
#+begin_src shell 
# iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE    # to activate NAT
# iptables -t nat -A PREROUTING -p tcp -d 78.31.70.238 --dport 20022 -j DNAT \
--to 192.168.16.44:22           # Port forward 20022 to internal IP port ssh
# iptables -t nat -A PREROUTING -p tcp -d 78.31.70.238 --dport 993:995 -j DNAT \
--to 192.168.16.254:993-995     # Port forward of range 993-995
# ip route flush cache
# iptables -L -t nat            # Check NAT status
#+end_src 

** DNS

The DNS entries are valid for all interfaces and are stored in /etc/resolv.conf.
The domain to which the host belongs is also stored in this file. A minimal configuration is:

nameserver 66.63.128.84
search cipi.net intern.lab
domain cipi.org

Check the system domain name with:

#+begin_src shell
# hostname -d                # Same as dnsdomainname
#+end_src 

** DHCP

#+begin_src shell 
# dhcpcd -n eth0           # Trigger a renew (does not always work)
# dhcpcd -k eth0           # release and shutdown
#+end_src 

The lease with the full information is stored in:
/var/lib/dhcpcd/dhcpcd-eth0.info

** tar
The command tar (tape archive) creates and extracts archives of file
and directories. The archive .tar is uncompressed, a compressed
archive has the extension .tgz or .tar.gz (zip) or .tbz (bzip2). Do
not use absolute path when creating an archive, you probably want to
unpack it somewhere else. Some typical commands are:

*** Create
Only include one (or two) directories from a tree, but keep the
relative structure. For example archive /usr/local/etc and
/usr/local/www and the first directory in the archive should be
local/.

 #+begin_src shell

$ tar czf name_of_archive_file.tar.gz name_of_directory_to_tar 
$ tar -C /usr -czf local.tgz local/etc local/www
$ tar -C /usr -xzf local.tgz      # To untar the local dir into /usr
$ cd /usr; tar -xzf local.tgz     # Is the same as above

$ cd /var/www && sudo tar czf ~/www_backups/$(date +%Y%m%d-%H%M%S).tar.gz .
# This would have created a file named something like 20120902-185558.tar.gz.
 #+end_src
 
*** Extract

 #+begin_src shell
$ tar -tzf home.tgz               # look inside the archive without extracting (list)
$ tar -xf home.tar                # extract the archive here (x for extract)
$ tar -xzf home.tgz             # same with zip compression (-xjf for bzip2 compression)
                                # remove leading path gallery2 and extract into gallery
$ tar --strip-components 1 -zxvf gallery2.tgz -C gallery/
$ tar -xjf home.tbz home/colin/file.txt    # Restore a single file
# extracting bz2
$ tar xvjf file.tar.tbz 
 #+end_src
 
*** More advanced

#+begin_src shell
# tar c dir/ | gzip | ssh user@remote 'dd of=dir.tgz' # arch dir/ and store remotely.
# tar cvf - `find . -print` > backup.tar                 # arch the current directory.
# tar -cf - -C /etc . | tar xpf - -C /backup/etc      # Copy directories
# tar -cf - -C /etc . | ssh user@remote tar xpf - -C /backup/etc      # Remote copy.
# tar -czf home.tgz --exclude '*.o' --exclude 'tmp/' home/
#+end_src
 
** Miscellaneous

#+begin_src shell
$ which command                      # Show full path name of command
$ time command                         # See how long a command takes to execute
$ time cat                                     # Use time as stopwatch. Ctrl-c to stop
$ set | grep $USER                    # List the current environment
$ cal -3                                         # Display a three month calendar
$ date [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]]
$ date 10022155                       # Set date and time
$ whatis grep                              # Display a short info on the command or word
$ whereis java                            # Search path and standard directories for word
$ setenv varname value           # Set env. variable varname to value (csh/tcsh)
$ export varname="value"        # set env. variable varname to value (sh/ksh/bash)
$ pwd                                # Print working directory
$ mkdir -p /path/to/dir                 # no error if existing, make parent dirs as needed
$ mkdir -p project/{bin,src,obj,doc/{html,man,pdf},debug/some/more/dirs}
$ rmdir /path/to/dir                     # Remove directory
$ rm -rf /path/to/dir                     # Remove directory and its content (force)
$ rm -- -badchar.txt                    # Remove file whitch starts with a dash (-)
$ cp -la /dir1 /dir2                       # Archive and hard link files instead of copy
$ cp -lpR /dir1 /dir2                    #
$ cp unixtoolbox.xhtml{,.bak}  # Short way to copy the file with a new extension
$ mv /dir1 /dir2                           # Rename a directory
$ ls -1                                           # list one file per line
$ history | tail -50                       # Display the last 50 used commands
$ cd -                                            # cd to previous ($OLDPWD) directory
#+end_src
 
** Add/Remove software

Debian/Ubuntu/Mint

#+begin_src shell
$ apt-get update                     # First update the package lists
$ apt-get install emacs          # Install the package emacs
$ dpkg --remove emacs        # Remove the package emacs
$ dpkg -S file                           # find what package a file belongs to
#+end_src 

* git
** add a remote

#+begin_src shell
git remote add origin <repo-url>
#+end_src

** Clone git repository without history?
   
#+begin_src shell
$ git clone --depth 1 reponame.git
$ git clone --depth=1 <remote_repo_url>
#+end_src

** ignore files in a directory

#+begin_src 
# ignores all files in tmp directory
tmp/*
#+end_src

** add a remote to current repository 

#+begin_src shell
$ git remote add origin http://172.16.8.18/mansouri/xbs.git
#+end_src 

** git fails when pushing commit to github
   
#+begin_src shell
$ git config http.postBuffer 524288000
#+end_src 

** clone only a branch

#+begin_src shell
$ git clone  --branch release <git_address> 
#+end_src 

* font
** rebuild font cache
   
#+begin_src shell
# fc-cache -f -v <dir>  
# where <dir> is the directory to search for fonts
$ fc-cache -f -v ~/.fonts/adobe-fonts/source-code-pro
#+end_src 

** install =source code pro=

#+begin_src shell
#!/bin/sh

# ~/.fonts is now deprecated and that
#FONT_HOME=~/.fonts
# ~/.local/share/fonts should be used instead
FONT_HOME=~/.local/share/fonts

echo "installing fonts at $PWD to $FONT_HOME"
mkdir -p "$FONT_HOME/adobe-fonts/source-code-pro"
# find "$FONT_HOME" -iname '*.ttf' -exec echo '{}' \;

(git clone \
   --branch release \
   --depth 1 \
   'https://github.com/adobe-fonts/source-code-pro.git' \
   "$FONT_HOME/adobe-fonts/source-code-pro" && \
fc-cache -f -v "$FONT_HOME/adobe-fonts/source-code-pro")
#+end_src 

* gnome
** Ubuntu Gnome - force alt + tab to only switch on current workspace
http://askubuntu.com/questions/121126/can-i-alt-tab-windows-from-all-workspaces

Geborgenheit;;feeling of security
unersetzlich;;irreplaceable
lässig;;casual nonchalant, cool
es schwer;;haben to have a hard time
eichen;;to calibrate
Herzinfarkt;; heart attack
Pfeife;;pipe
ein Kind kriegen;;to have a baby
Zärtlichkeit (die);;fondness, loving affection
blöd;;stupid, dumb
Lüge (die);;lie, tale, untruth
allzeit;; always
furchtbar;;dreadfully, awfully, terribly,
einsam;;lonely, 
Streiter (der);;fighter, wrangler
Krieg (der);;war, 
sonderbar;;strange
egal;;the same, all the same
** change active tab color
in */usr/share/themes/Ambiance/gtk-3.0/gtk-widgets.css* change the line below
with the color you like

#+begin_src
/* give active tab a background, as it might be dragged across of others when reordering */
.notebook tab:active {
    /*background-color: @bg_color;*/
    background-color: #def;
}
#+end_src 

* disk
** partition a disk
** list partitions

#+begin_src shell 
$ sudo fdisk -l        #shows all partitions
$ sudo fdisk -l /dev/sda
#+end_src 

** make partitions
entering command mode in fdisk

#+begin_src shell 
$ sudo fdisk /dev/sda
#+end_src 

then type n for new partition.

** format a partition

#+begin_src shell
mkfs -v -t ext4 /dev/<xxx>
#+end_src 

** make a swap partition

#+begin_src shell
mkswap /dev/<yyy>
#+end_src 
http://www.tldp.org/HOWTO/Partition/fdisk_partitioning.html

** auto mount a partition
 
Once a file system is actually mounted , an entry for it is made
by the operating system in the */etc/mtab* file.
Automatic mounts are handled by configuration the */etc/fstab* file.

An entry in an fstab file contains several fields, each
separated by a space or tab.

** find UUID of a filesystem

Look up data on /dev/sda1:

#+begin_src shell
topher@crucible:~$ sudo blkid /dev/sda1
/dev/sda1: UUID="727cac18-044b-4504-87f1-a5aefa774bda" TYPE="ext3"
#+end_src 

Show UUID data for all partitions:

#+begin_src shell 
topher@crucible:~$ sudo blkid
/dev/sda1: UUID="727cac18-044b-4504-87f1-a5aefa774bda" TYPE="ext3"
/dev/sdb: UUID="467c4aa9-963d-4467-8cd0-d58caaacaff4" TYPE="ext3"
#+end_src 

Show UUID data for all partitions in easier to read format: (Note: in newer
releases, blkid -L has a different meaning, and blkid -o list should be used
instead)

#+begin_src shell 
topher@crucible:~$ sudo blkid -L
device     fs_type label    mount point    UUID
-------------------------------------------------------------------------------
/dev/sda1 ext3             /              727cac18-044b-4504-87f1-a5aefa774bda
/dev/sdc  ext3             /home          467c4aa9-963d-4467-8cd0-d58caaacaff4
Show just the UUID for /dev/sda1 and nothing else:

topher@crucible:~$ sudo blkid -s UUID -o value /dev/sda1
727cac18-044b-4504-87f1-a5aefa774bda
#+end_src 

* shell
** login shell vs non-login shell?
   
When you sit at a terminal and enter a username and password in
response to a prompt from the computer, you get a login
shell. Similarly, when you use ssh hostname, you get a login
shell. However, if you run a shell by name, or implicitly as the
command interpreter named in the initial #! line in a script, or
create a new workstation terminal window, or run a command in a remote
shell with /for example, ssh hostname command/ then that shell is
not a login shell.

*** How to check if the shell is a login shell?
The shell determines whether it is a login shell by examining the
value of $0. If the value begins with a hyphen, then the shell is a
login shell; otherwise, it is not. You can tell whether you have a
login shell by this simple experiment:

#+begin_src shell
$ echo $0                                  Display shell name
-ksh                                      Yes, this is a login shell
#+end_src

*** bash login shell startup?
When bash is a login shell, on startup it does the equivalent of: 

#+begin_src shell
test -r /etc/profile && . /etc/profile              Try to read /etc/profile

if test -r $HOME/.bash_profile ; then               Try three more possibilities

    . $HOME/.bash_profile

elif test -r $HOME/.bash_login ; then

    . $HOME/.bash_login

elif test -r $HOME/.profile ; then

    . $HOME/.profile

fi
#+end_src

*** bash non-login shell initilization?

Unlike the Bourne shell, bash reads an initialization file on startup
when it is an interactive nonlogin shell, by steps equivalent to this:

#+begin_src shell
test -r $HOME/.bashrc && . $HOME/.bashrc            Try to read $HOME/.bashrc
#+end_src

** change history size?
for ubuntu change ~/.bashrc file variables ~HISTSIZE~ & ~HISTFILESIZE~
** adding to path to ~$PATH~ envrionment variable
append to */etc/environment*
 - works for non-login shells but not for login-shells

append to */etc/profile*
 - works for login-shells only

append to *~/.bashrc*
 - works only for none-login shells

create file at */etc/profile.d* and add a file with *sh* (important) extension eg:

PATH=/opt/anaconda3/bin:$PATH

 - this probably only works in non-login shells

change default path for users at */etc/login.defs*

#+begin_src shell
ENV_SUPATH      PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin    # for super users
ENV_PATH        PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games        
#+end_src

create *~/.bash_profile* and call *~/.bashrc* file like this
#+begin_src shell
[[ -r ~/.bashrc ]] && . ~/.bashrc
#+end_src


*order of bash login init files*

#+begin_src shell
/bin/bash
       The bash executable
/etc/profile
       The systemwide initialization file, executed for login shells
~/.bash_profile
       The personal initialization file, executed for login shells.Would be used only once, at login.
~/.bashrc
       The individual per-interactive-shell startup file.
~/.bash_logout
       The individual login shell cleanup file, executed when a login shell exits.
~/.inputrc
       Individual readline initialization file.
#+end_src

* Toolchain
** Linker
*** dynamic linker, often referred to as dynamic loader vs standard linker *ld*
Also be aware of the name of the platform's dynamic linker, often
referred to as the dynamic loader (not to be confused with the
standard linker ld that is part of Binutils). The dynamic linker
provided by Glibc finds and loads the shared libraries needed by a
program, prepares the program to run, and then runs it. The name of
the dynamic linker for a 32-bit Intel machine will be ld-linux.so.2
(ld-linux-x86-64.so.2 for 64-bit systems). A sure-fire way to
determine the name of the dynamic linker is to inspect a random binary
from the host system by running: *readelf -l <name of binary> | grep interpreter*
and noting the output. The authoritative reference
covering all platforms is in the shlib-versions file in the root of
the Glibc source tree.
*** Linker search path
    
#+begin_src shell
$ ld --verbose | grep SEARCH
#+end_src 
will illustrate the current search paths and their order.

*** To find out which standard linker gcc will use, run: 

#+begin_src shell
$ gcc -print-prog-name=l
#+end_src

* Text processing
** cut
*** example inputs

#+begin_src shell 
> cat file.txt
unix or linux os
is unix good os
is linux good os
#+end_src

*** Write a unix/linux cut command to print characters by position?

#+begin_src shell
cut -c4 file.txt
x
u
l
#+end_src

The above cut command prints the fourth character in each line of the file
*** Write a unix/linux cut command to print characters by range?

#+begin_src shell
cut -c4-7 file.txt
x or
unix
linu
#+end_src

*** print the first six characters in a line

#+begin_src shell
cut -c-6 file.txt
unix o
is uni
is lin
#+end_src

*** To print the characters from tenth position to the end

#+begin_src shell
cut -c10- file.txt
inux os
ood os
good os
#+end_src

*** Write a unix/linux cut command to print the fields using the delimiter?
    
#+begin_src shell 
cut -d' ' -f2 file.txt
or
unix
linux
#+end_src

*** prints the second and third field in each line.

#+begin_src shell 
cut -d' ' -f2,3 file.txt
or linux
unix good
linux good
#+end_src

*** Write a unix/linux cut command to display range of fields?

You can print a range of fields by specifying the start and end position.

#+begin_src shell 
cut -d' ' -f1-3 file.txt
#+end_src
 
The above command prints the first, second and third fields.

*** cut by new line?

#+begin_src shell
cat textfile | cut -f3 -d$'\n'
#+end_src

** tr
*** Replace multiple spaces with one using 'tr' only
With tr, use the squeeze repeat option:

#+begin_src shell
$ tr -s " " < file
#+end_src

** sed
*** useful resources
http://sed.sourceforge.net/

http://sed.sourceforge.net/sed1line.txt

** remove empty lines
#+begin_src shell
$ sed '/^\s*$/d'
$ awk 'NF' file
#+end_src

* processes
** checking the priority of a process?

#+begin_src 
ps -o pid,comm,nice -p 594
#+end_src
 
** Setting priority on new processes

#+begin_src 
nice -n 10 apt-get upgrade 
#+end_src
 
This will increment the default nice value
by a positive 10 for the command, ‘apt-get upgrade’ This is often
useful for times when you want to upgrade apps but don’t want the
extra process burden at the given time.

** Setting Priority on Existing Processes

#+begin_src
renice 10 -p 21827
#+end_src

** Setting Permanent Priority on all Processes for a Specific User

Sometimes it is helpful to give specific users lower priority than
others to keep system resources allocated in the proper places like
core services and other programs.

You can set the default nice value of a particular user or group in
the /etc/security/limits.conf file.

#+begin_src
/etc/security/limits.conf
#+end_src

It uses this syntax: [username] [hard|soft] priority [nice value]

#+begin_src
backupuser hard priority 1
#+end_src

* tor
** change ip

#+begin_src shell
printf "AUTHENTICATE \"password\"\r\nSIGNAL NEWNYM\r\n" | nc 127.0.0.1 9051
#+end_src

yet another way

#+begin_src shell
service tor reload
#+end_src
** use tor in shell 
#+begin_src shell
$ . torify on
#+end_src

* android
** sdkmanager
#+begin_src shell 
$ ./sdkmanager --proxy=http  --no_https --proxy_host=127.0.0.1 --proxy_port=8118 --list
#+end_src 

* multimedia
** convert avi to mp4

#+begin_src shell
avconv -i test.avi -c:v libx264 -c:a copy outputfile.mp4
#+end_src

use the ~-threads~ switch to control the number of threads

#+begin_src shell
avconv -i test.avi -c:v libx264 -c:a copy -threads 1  outputfile.mp4
#+end_src

* syslog
** debugging
 - run the script every 1 minute
 - make sure that the cron logs to syslog(or rsyslog). In ubuntu it's disabled by default
   and it's located at */etc/rsyslog*

The easiest way is simply to send all STDOUT and STDERR to Syslog

#+begin_src shell
    * * * * * echo "test message" 2>&1 |logger

#If you want to debug your bash script just add debug mode to the beginning of your script
set -x

#To ensure your jobs are executed tail on /var/log/cron
$ tail -f /var/log/cron

# To see all the outputs in Syslog
$ tail -f /var/log/messages

#+end_src 

http://www.emind.co/how-to/how-to-debug-cron-jobs/

* number crunching
** using bc to show control the number of digits after the decimal point?

use the ~scale~ special variable

#+begin_src shell
echo "scale=2; 100/3" | bc
#+end_src

* make a ramdisk?

The tmpfs filesystem is a RAMDISK.

#+begin_src shell
sudo mkdir -p /media/ramdisk
sudo mount -t tmpfs -o size=2048M tmpfs /media/ramdisk
#+end_src

http://askubuntu.com/questions/152868/how-do-i-make-a-ram-disk
* curl
** Fetching a Page with cURL

#+begin_src shell
# basic invocation
curl -o example.html http://www.example.com/
# fetch a secure web page
curl -k -o example-secure.html https://www.example.com/
# fetch a file by FTP. This time, have curl automatically
# pick the output filename
curl -O ftp://ftp.example.com/pub/download/file.zip
#+end_src

** Fetching Many Variations on a URL

#+begin_src shell
# Fetch all the categories from 00 to 99.
curl -o 'category-#1#2.html' 'http://www.example.com/category.php?CATID=[0-9][0-9]'
curl -o 'category-#1.html' 'http://www.example.com/category.php?CATID=[0-99]'
# Fetch several main pages and store them in files named accordingly
curl -o '#1.html' 'http://www.example.com/{news,blog,careers,contact,sitemap}/'
#+end_src

** Following Redirects Automatically

#+begin_src shell
curl -L -e ';auto' -o 'output.html' 'http://www.example.com/login.jsp'
#+end_src
 
You typically need to use a combination of -L and -e
';auto' simultaneously to achieve the effect you want. The -L option
tells cURL to follow redirect responses. The -e ';auto' option tells
it to pass the Referer header when it follows them. This more closely
matches the behavior of real web browsers.

** send cookie with curl?

#+begin_src shell
curl -v --cookie "USER_TOKEN=Yes" http://127.0.0.1:5000/
#+end_src

** make an options request

#+begin_src shell	
curl -i -X OPTIONS http://example.org/path
#+end_src 

** make a head request

#+begin_src shell
curl --head http://example.org
#+end_src 

** make an options request

#+begin_src shell
curl -i -X OPTIONS http://example.org/path
#+end_src 

** set a header

#+begin_src shell
curl --header "X-MyHeader: 123" www.google.com
echo "0217"$(date +%Y-%m-%d-%H-%M-%S-%N) | tr -d "-" | php -r 'echo substr(file("php://stdin")[0],0,20);'
#+end_src 

** post request

#+begin_src shell
#With fields:

curl --data "param1=value1&param2=value2" https://example.com/resource.cgi

#Multipart:

curl --form "fileupload=@my-file.txt" https://example.com/resource.cgi

#Multipart with fields and a filename:

curl --form "fileupload=@my-file.txt;filename=desired-filename.txt" --form param1=value1 --form param2=value2 https://example.com/resource.cgi

#Without data:

curl --data '' https://example.com/resource.cgi

curl -X POST https://example.com/resource.cgi

curl --request POST https://example.com/resource.cgi

#For large files, consider adding parameters to show upload progress:

curl --tr-encoding -X POST -v -# -o output -T filename.dat  http://example.com/resource.cgi

#The -o output is required, otherwise no progress bar will appear.

#+end_src 

** get cookie
#+begin_src shell
$ curl -c 'http://example.com/'
# session flood
$ while true; do curl -c - 'http://localhost/xbs/login' 1> /dev/null; done 
#+end_src

** set useragent string
   
#+begin_src shell
# To make curl look like Internet Explorer 5 on a Windows 2000 box:
$ curl --user-agent "Mozilla/4.0 (compatible; MSIE 5.01; Windows NT 5.0)" [URL]
#Or why not look like you're using Netscape 4.73 on an old Linux box:
$ curl --user-agent "Mozilla/4.73 [en] (X11; U; Linux 2.2.15 i686)" [URL]
#+end_src

** links

https://curl.haxx.se/docs/httpscripting.html
* grub
** grub change timeout

#+begin_src shell 
$ sudo vim /etc/default/grub 
#+end_src 	

 and set the *GRUB_TIMEOUT*. 
 -1 will disable it. And then run

#+begin_src shell 
$ sudo update-grub
#+end_src 

** Repair grub

So you broke grub? Boot from a live cd, [find your linux partition
under /dev and use fdisk to find the linux partion] mount the linux
partition, add /proc and /dev and use grub-install /dev/xyz. Suppose
linux lies on /dev/sda4:

 #+begin_src shell
 # mount /dev/sda6 /mnt                   # mount the linux partition on /mnt
 # mount --bind /proc /mnt/proc       # mount the proc subsystem into /mnt
 # mount --bind /dev /mnt/dev          # mount the devices into /mnt
 # chroot /mnt                                      # change root to the linux partition
 # grub-install /dev/sda                     # reinstall grub with your old settings
 #+end_src

* listing broken packages?

#+begin_src shell
$ sudo apt-get check
#+end_src
 
* completely remove a package with configurations

#+begin_src shell	
$ sudo apt-get purge <package_name>
#+end_src 	

* deleting broken packages?

use synaptic package manager.

#+begin_src shell 
$ sudo dpkg -P package_name			# -P for purge
#+end_src 

* sudo timeout

use =visudo= to edit =/etc/sudoers= file. It validates the file upon exit and
locks the file while it's being edited.

#+begin_src shell
$ sudo visudo
#+end_src
 
to increase the timeout to 30 minutes for user jsmith, you would put
in a line as follows at the bottom of the file:

#+begin_src
Defaults:jsmith timestamp_timeout=30
#+end_src

The timestamp_timeout defines the number of minutes that can elapse
before sudo will ask for a password again.

~timestamp_timeout=0~ makes the sudo password to expire every 0(zero) seconds.
~timestamp_timeout=-1~ makes the suo password not expire.  

*increasing timeout*
You can extend the timeout for another 5minutes(or whatever the value of ~timestamp_timeout=-1~
is for you) using =sudo -v=.

see more : ~man 5 sudoers~

* How can I get the recoll package to index markdown (.md) files?

Edit =~/.recoll/mimemap=, add the following line:

~.md = text/plain~

This will tell recoll to index markdown as normal text, which it is, mostly, 
so I think that things should "just work".

* json pretty print
  
#+begin_src shell
$ cat some.json | python -m json.tool
#+end_src 

* installation
** other useful php modules

#+begin_src shell 
$ sudo apt-get install php-soap
$ sudo apt-get install php-ssh2
$ sudo apt-get install php-cli
$ sudo apt-get install php-mbstring
#+end_src 

** starting windows in safe mode from grub

for windows xp or 7 repeatedly press *F8* when you select the windows
item.
	
** making windows usb boot in ubuntu

#+begin_src shell
$ sudo apt-get install unetbootin
#+end_src 

** ubuntu installation

for installattion make an *ext4* partition as the primary and a *swap*
partition as logical drive.

when ubuntu is installed run *sudo update-grub* if the windows is not
shown in the grub boot list.

** tell which package does a file belong to?

#+begin_src shell 	
$ dpkg -S libgthread-2.0.so.0
#+end_src 

** apt-cacher-ng
*** installing apt-cacher-ng

add *00aptproxy* to */etc/apt/apt.conf.d/* and add the following lines

#+begin_src
Acquire::http::Proxy "http://127.0.0.1:3142";
#+end_src
	

 make *_import* folder in */var/cache/apt-cache-ng/_import* copy your
 deb files in *_import* and and goto *localhost:3142* and hit import.

*** precaching for ubuntu xenial

add =PrecacheFor: uburep/dists/xenial/*/binary-amd64/Packages*= to the
PreCache section located at =/etc/apt-cacher-ng/acng.conf=.

for scheduling the process use
#+begin_src shell
wget "http://localhost:3142/acng-report.html?abortOnErrors=aOe&calcSize=cs&doDownload=dd&doMirror=Start+Mirroring#bottom"
#+end_src

to start apt-cacher-ng in foreground
#+begin_src shell
$ sudo apt-cacher-ng -c /etc/apt-cacher-ng/ Port=3142 ForeGround=1 VerboseLog=1
#+end_src

** installing postgresql in ubuntu 16.0

#+begin_src shell 
$ sudo apt-get install postgresql postgresql-contrib
#+end_src 

Now that we can connect to our PostgreSQL server, the next step is to
set a password for the postgres user. Run the following command at a
terminal prompt to connect to the default PostgreSQL template
database:
#+begin_src shell 
$ sudo -u postgres psql template1
#+end_src 

The above command connects to PostgreSQL database template1 as user
postgres. Once you connect to the PostgreSQL server, you will be at a
SQL prompt. You can run the following SQL command at the psql prompt
to configure the password for the user postgres.
#+begin_src sql 
ALTER USER postgres with encrypted password 'your_password';
#+end_src 

Upon installation Postgres is set up to use ident authentication,
which means that it associates Postgres roles with a matching
Unix/Linux system account.

The installation procedure created a user account called postgres that
is associated with the default Postgres role.  Switch over to the
postgres account on your server by typing:

#+begin_src shell 
$ sudo -i -u postgres
#+end_src  

You can now access a Postgres prompt immediately by typing:
#+begin_src shell 
psql
#+end_src 

** php modules for connecting to postgresql
   
connecting to *postgresql* with *PDO*

#+begin_src shell 
$ sudo apt-get install php-pgsql
#+end_src 

Or if the package is installed, you need to enable the module in php.ini

#+begin_src shell 
extension=php_pgsql.dll (windows)
extension=php_pgsql.so (linux)
#+end_src 

** phpstorm bad gateway in phpstorm

try installing

#+begin_src shell 
$ sudo apt-get install php-cgi
#+end_src 

** nodjs

 How to install Node.js via binary archive on Linux?

 Unzip the binary archive to any directory you wanna install Node, I use /usr/lib/nodejs

     sudo mkdir /usr/lib/nodejs
     sudo tar -xJvf node-v6.5.0-linux-x64.tar.xz -C /usr/lib/nodejs
     sudo mv node-v6.5.0-linux-x64 node-v6.5.0
     Set the environment variable ~/.profile, add below to the end

     # Nodejs
     export NODEJS_HOME=/usr/lib/nodejs/node-v6.5.0
     export PATH=$NODEJS_HOME/bin:$PATH

 Test installation using

     $ node -v

     $ npm version

     the normal output is:

     ➜  nodejs node -v
     v6.5.0
     ➜  nodejs npm version
     { npm: '3.10.3',
     ares: '1.10.1-DEV',
     http_parser: '2.7.0',
     icu: '57.1',
     modules: '48',
     node: '6.5.0',
     openssl: '1.0.2h',
     uv: '1.9.1',
     v8: '5.1.281.81',
     zlib: '1.2.8' }

** nvidia binary driver screen flickering

Install Compiz Config, from a terminal, type:

#+begin_src shell 
$ sudo apt-get install compizconfig-settings-manager
#+end_src 

From the launcher, execute CompizConfig Settings Manager Check the
checkbox in "Utility -> Workarounds -> Force full screen redraws
(buffer swap) on repaint"

* postgresql
** installation

#+begin_src shell
$ sudo apt-get update
$ sudo apt-get install postgresql postgresql-contrib
#+end_src

** Switching Over to the postgres Account

#+begin_src shell
sudo -i -u postgres
#+end_src


*** refs
https://www.digitalocean.com/community/tutorials/how-to-install-and-use-postgresql-on-ubuntu-16-04

** Accessing a Postgres Prompt Without Switching Account

#+begin_src shell
$ sudo -u postgres psql
#+end_src
 
** creating a new role with =createuser=
If you are logged in as the postgres account, you can create a new user by typing:

#+begin_src shell
$ createuser --interactive
#+end_src
 
If, instead, you prefer to use sudo for each command without switching
from your normal account, you can type:

#+begin_src shell
$ sudo -u postgres createuser --interactive
#+end_src
 
The script will prompt you with some choices and, based on your
responses, execute the correct Postgres commands to create a user to
your specifications.

#+begin_src 
Output
Enter name of role to add: sammy
Shall the new role be a superuser? (y/n) y
#+end_src
 
** creating a new db with =createdb=
   
By default, another assumption that the Postgres authentication system
makes is that there will be an database with the same name as the role
being used to login, which the role has access to.

So if in the last section, we created a user called sammy, that role
will attempt to connect to a database which is also called sammy by
default. You can create the appropriate database with the createdb
command.

If you are logged in as the postgres account, you would type something like:

#+begin_src shell
postgres@server:~$ createdb sammy
#+end_src
 
If, instead, you prefer to use sudo for each command without switching
from your normal account, you would type:

#+begin_src shell
sudo -u postgres createdb sammy
#+end_src
 
** psql
*** list all dbs and users?
\l
*** PostgreSQL “DESCRIBE TABLE”
\d+ tablename

** create autoincrement column
The data types serial and bigserial are not true types, but merely a
notational convenience for creating unique identifier columns (similar
to the AUTO_INCREMENT property supported by some other databases). In
the current implementation, specifying:

#+begin_src sql
CREATE TABLE tablename (
    colname SERIAL
);
#+end_src


is equivalent to specifying:

#+begin_src sql
CREATE SEQUENCE tablename_colname_seq;
CREATE TABLE tablename (
    colname integer NOT NULL DEFAULT nextval('tablename_colname_seq')
);

ALTER SEQUENCE tablename_colname_seq OWNED BY tablename.colname;
#+end_src

** Accessing a Postgres Prompt Without Switching Accounts
** backup with pg_dump

#+begin_src shell
$ /usr/local/bin/pg_dump shahkar -U pgsql > /root/farahoosh/shahkar-db-backup/`date +%Y-%m-%d-%H:%M:%S`.sql
#+end_src 

You can also run the command you'd like with the postgres account
directly with sudo.

#+begin_src shell
$ sudo -u postgres psql
#+end_src

** create UUID without extension
   
#+begin_src sql
SELECT uuid_in(md5(random()::text || now()::text)::cstring);
#+end_src

http://stackoverflow.com/questions/12505158/generating-a-uuid-in-postgres-for-insert-statement

** get list of installed extensions?
   
#+begin_src sql
SELECT * FROM pg_available_extensions;
#+end_src

** copying postgresql database to another server

#+begin_src shell
$ pg_dump -C -h localhost -U localuser dbname | psql -h remotehost -U remoteuser dbname
# from remote host
$ pg_dump -C -h remotehost -U remoteuser dbname | psql -h localhost -U localuser dbname
# with compression
$ pg_dump -C dbname | bzip2 | ssh  remoteuser@remotehost "bunzip2 | psql dbname"
$ pg_dump -C dbname | ssh -C remoteuser@remotehost "psql dbname"
#+end_src

** active connections to the database

#+begin_src sql
select * from pg_stat_activity;
#+end_src 

** rename postgresql database
   
#+begin_src sql
alter <db_name> rename to <new_db_name>;
#+end_src 

** which version of =postgresql= am I running?

#+begin_src shell

$ pg_config --version
#Client version:
$ psql --version

#  using psql
$psql
postgres=# \g
postgres=# SELECT version();

#+end_src 

** restoring database with =psql=

#+begin_src shell 
$ createdb dbname
$ cat filename* | psql dbname
#+end_src 

https://www.postgresql.org/docs/8.1/static/backup.html

** connect to =postgresql= database in local host without password

#+begin_src shell 
$ locate pg_hba.conf
$ vim /path/to/pg_hba.conf
#+end_src 

change METHOD to =trust=

#+begin_src 
TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             127.0.0.1/32            trust
#+end_src 

** How to reload config settings without restarting database

if you are making modifications to the Postgres configuration file
postgresql.conf (or similar), and you want to new settings to take effect
without needing to restart the entire database, there are two ways to accomplish
this.

Option 1: From the command-line shell

#+begin_src shell
$ su - postgres
$ /usr/bin/pg_ctl reload
#+end_src 

Option 2: Using SQL

#+begin_src 
SELECT pg_reload_conf();
#+end_src 

** backup
*** intro
*Logical backups*: A logical backup refers to the dump file that is created by the
pg_dump utility and which might be used to restore the database in the case of a
data loss or an accidental deletion of a database object, such as a table. The
pg_dump utility is a PostgreSQL specific utility that can be run on the command
line, which makes a connection to the database and initiates the logical backup.

*Physical backups*: A physical backup refers to the OS level backup of a database
directory and its associated files.

*** backup a single database

#+begin_src shell
$ pg_dump -U username -W -F t database_name > [Backup Location Path]
#+end_src 

The usage of the options used with the pg_dump command is explained here:

*U switch*: The -U switch specifies the database user initiating the connection.
As pg_dump is a command-line utility, we need to specify the username via which
the pg_dump utility can make a database connection.

*W switch*: This option is not mandatory. This option forces pg_dump to prompt for
the password before connecting to the PostgreSQL database server. After you
press Enter, pg_dump will prompt for the password of the database user from
which the connection is initiated.

*F switch*: The -F switch specifies the output file format that will be used. We
specified the t option with the -F switch because the output file will be
implemented as a tar format archive file.
*** logical backup of all databases

to back up all the databases in one go in Linux, use the pg_dumpall command, as
follows:

#+begin_src shell
$ pg_dumpall -U postgres >   /home/pgbackup/all.sql
#+end_src 

To back up all object definitions in all the databases, including roles,
tablespaces, databases, schemas, tables, indexes, triggers, functions,
constraints, views, ownership, and privileges, you can use the following command
in Windows:

#+begin_src shell
$fpg_dumpall --schema-only > c:\pgdump\definitiononly.sql
#+end_src 

If you want to back up the role definition only, use the following command:

#+begin_src shell
$ pg_dumpall --roles-only > c:\pgdump\myroles.sql
#+end_src 

If you want to back up tablespace definitions, use the following command:

#+begin_src shell
$ pg_dumpall --tablespaces-only > c:\pgdump\mytablespaces.sql
#+end_src 

*** taking a base backup
You can use the pg_basebackup command in the following manner:

#+begin_src shell
$ pg_basebackup -h 192.168.10.14 -D /home/abcd/pgsql/data
#+end_src 

Here, we take a base backup of the server located at 192.168.10.14 and store it
in the /home/abcd/pgsql/data local directory.
** examplg =pg_hba.conf=
   
#+begin_src 
Example pg_hba.conf Entries

# Allow any user on the local system to connect to any database with
# any database user name using Unix-domain sockets (the default for local
# connections).
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             all                                     trust

# The same using local loopback TCP/IP connections.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             127.0.0.1/32            trust

# The same as the previous line, but using a separate netmask column
#
# TYPE  DATABASE        USER            IP-ADDRESS      IP-MASK             METHOD
host    all             all             127.0.0.1       255.255.255.255     trust

# The same over IPv6.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             ::1/128                 trust

# The same using a host name (would typically cover both IPv4 and IPv6).
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             localhost               trust

# Allow any user from any host with IP address 192.168.93.x to connect
# to database "postgres" as the same user name that ident reports for
# the connection (typically the operating system user name).
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    postgres        all             192.168.93.0/24         ident

# Allow any user from host 192.168.12.10 to connect to database
# "postgres" if the user's password is correctly supplied.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    postgres        all             192.168.12.10/32        md5

# Allow any user from hosts in the example.com domain to connect to
# any database if the user's password is correctly supplied.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             .example.com            md5

# In the absence of preceding "host" lines, these two lines will
# reject all connections from 192.168.54.1 (since that entry will be
# matched first), but allow Kerberos 5 connections from anywhere else
# on the Internet.  The zero mask causes no bits of the host IP
# address to be considered, so it matches any host.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             192.168.54.1/32         reject
host    all             all             0.0.0.0/0               krb5

# Allow users from 192.168.x.x hosts to connect to any database, if
# they pass the ident check.  If, for example, ident says the user is
# "bryanh" and he requests to connect as PostgreSQL user "guest1", the
# connection is allowed if there is an entry in pg_ident.conf for map
# "omicron" that says "bryanh" is allowed to connect as "guest1".
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    all             all             192.168.0.0/16          ident map=omicron

# If these are the only three lines for local connections, they will
# allow local users to connect only to their own databases (databases
# with the same name as their database user name) except for administrators
# and members of role "support", who can connect to all databases.  The file
# $PGDATA/admins contains a list of names of administrators.  Passwords
# are required in all cases.
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   sameuser        all                                     md5
local   all             @admins                                 md5
local   all             +support                                md5

# The last two lines above can be combined into a single line:
local   all             @admins,+support                        md5

# The database column can also use lists and file names:
local   db1,db2,@demodbs  all                                   md5
#+end_src

** connect to =psql= without password 

After installation, open ~<PostgreSQL PATH>\data\pg_hba.conf~.
Modify these two lines, and change "md5" to "trust":

#+begin_src
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
#+end_src 

* phppgadmin

*installing phppgadmin and configuring Postgresql user*
#+begin_src shell
$ sudo apt-get install phppgadmin
$ sudo su posgtres                    #login as postgres
$ psql                
$ \password posgres                   # change password for postgres role
$ \q
#+end_src

*configuring apache2*

#+begin_src shell
$ cd /etc/apache2/conf-available/
$ nano phppgadmin.conf
#+end_src
 
Comment out the line =#Require local= by adding a # in front of the line
and add below the line =allow from all= so that you can access from your
browser.

*configuring phppgadmin*
Edit the file /etc/phppgadmin/config.inc.php by typing :

#+begin_src shell
cd /etc/phppgadmin/
nano config.inc.php
#+end_src
 
Find the line =$conf['extra_login_security'] = true;= and change the
value to false so you can login to phpPgAdmin with user postgres.

#+begin_src shell
systemctl restart postgresql
systemctl restart apache2
#+end_src

* working with Base64
** Decode a string

#+begin_src shell 
% echo 'Q29uZ3JhdHVsYXRpb25zIQ==' | openssl base64 -d
#+end_src 

** Encode the entire contents of a file
   
#+begin_src shell 
% openssl base64 -e -in input.txt -out input.b64
#+end_src 

This puts the Base 64-encoded output in a file called input.b64. 

** Encode a simple string

#+begin_src shell 
% echo -n '&a=1&b=2&c=3' | openssl base64 -e
#+end_src 

* calculating hashes
** md5

#+begin_src shell
% echo -n "my data" | openssl md5
#+end_src

** sha-1

#+begin_src perl
#/usr/bin/perl
use Digest::SHA1  qw(sha1);
$data   = "my data";
$digest = sha1($data);
print "$digest\n";
#+end_src
 


sudo apt-cacher-ng -c /etc/apt-cacher-ng/ Port=3142 ForeGround=1 VerboseLog=1
wget "http://localhost:3142/acng-report.html?abortOnErrors=aOe&calcSize=cs&doDownload=dd&doMirror=Start+Mirroring#bottom"

#+begin_src ditaa :file img/ditaa-simpleboxes.png
+---------+
|         |
| Foo     |
|         |
+----+----+---+
|Bar |Baz     |
|    |        |
+----+--------+
#+end_src

* unicode
** deocde a utf escaped string

use =echo -en= 
#+begin_src shell
$ echo -en "\u0622\u0642\u0627\u06cc"
#+end_src 
http://stackoverflow.com/questions/8795702/how-to-convert-uxxxx-unicode-to-utf-8-using-console-tools-in-nix

* wireless
** concepts
*** wlan frames
 1 *Management frames*: Management frames are responsible for maintaining
 communication between access points and wireless clients. Management frames
 can have the following subtypes:

  - Authentication
  - Deauthentication
  - Association request
  - Association response
  - Reassociation request
  - Reassociation response
  - Disassociation
  - Beacon
  - Probe request
  - Probe response

 2. *Control frames*: Control frames are responsible for ensuring a proper exchange
 of data between access points and wireless clients. Control frames can have the
 following subtypes:

  - Request to Send (RTS)
  - Clear to Send (CTS)
  - Acknowledgement (ACK)

 3. *Data frames*: Data frames carry the actual data that is sent on the wireless network.
 There are no subtypes for data frames.

*** Shared Key Authentication
 Shared Key Authentication uses a shared secret such as the WEP key to authenticate the
 client.

 #+begin_src ditaa :file img/shared-authentication.png
 +---------+                +---------+
 |cPNK{io} |                |         |
 +         +                +         +
 | client  |--------------> | Access  |
 +         +                + Point   +
 |         |                |         |
 +---------+                +---------+
     ^                           ^
     | ------------------------> |
     | 1)Authentication request  |
     |                           |
     | <------------------------ |
     | 2) AP sends challenge text|
     |                           |
     | ------------------------> |
     | 3) challenge response     |
     |                           |
     | <------------------------ |
     | 4) Authentication success |
     |    failure                |
 #+end_src

 The security problem here is that an attacker passively listening to this entire communication
 by sniffing the air has access to both the plain text challenge and the encrypted challenge. He
 can apply the XOR operation to retrieve the keystream. This keystream can be used to encrypt
 any future challenge sent by the access point without needing to know the actual key.

 The most common form of shared authentication is known as WEP or Wired Equivalent
 Protocol.

** aircrack-ng
*** put card in monitor mode

 #+begin_src shell
 $ sudo airmon-ng start <card_name>
 # now  you should be able to see mon0 with ifconfig
 $ ifconfig mon0              
 # stopping monitor mode
 $ sudo airmon-ng stop mon0
 #+end_src 

*** check for interfering processes

#+begin_src shell 
$ airmon-ng check        # check for interfering processes
$ airmon-ng check kill   # kill them if necessary
#+end_src 

*** capturing packets

#+begin_src shell 
$ airodump-ng mon0 --channel 6
#+end_src 

*** viewing management,control and data frames

usig wireshark enter ~wlan.fc.type==0~ to view management frames.
enter ~wlan.fc.type==1~ to view control frames.
enter ~wlan.fc.type==2~ to view data frames.

To additionaly select a *subtype* use ~wlan.fc.subtype~ filter.
For example to view all the Beacon frame among Manangement Frames,
use ~(wlan.fc.type == 0) && (wlan.fc.subtype == 8)~.

*** sniffing data packets of a network.

 *sniffing a specific access point*
 use ~airodump-ng --bssid <mac> mon0~ where <mac>, is the MAC address of 
 the access point we are trying to sniff.

 *lock the wireless card on a specific channel*
 ~iwconfig mon0 channel 11~ locks the wirless card on channel 11. In order
 to verify it run ~iwconfig mon0~

 *use wireshark to sniff the packets*
 ~wlan.bssid==<mac>~ where <mac> is the mac of the target access point.

*** bypassing authentication
**** finding hidden SSID

 *finding <mac> of hidden SSID*
 find the <mac> of the target with hidden SSID.We will wait for a
 legitimate client to connect to the access point with the hidden 
 SSID. This will generate a probe request and probe response packets
 that will contain the SSID of the network, thus revealing its presence.

 use ~wlan.addr == <mac>~, where mac is the mac address of the target with
 hidden SSID.

 *sending deauthentication packets*
 Alternately, you can use the aireplay-ng utility to send deauthentication packets
 to all stations on behalf of the Wireless Lab access point by typing.

 #+begin_src shell
 $ aireplay-ng -0 5 -a <mac> --ignore-negative mon0
 #+end_src 

 where <mac> is the MAC address of the router. The *-0* option is used to choose a
 deauthentication attack, and 5 is the number of deauthentication packets to
 send.Finally, *-a* specifies the MAC address of the access point you are
 targeting.

 The preceding deauthentication packets will force all legitimate clients to
 disconnect and reconnect.

 *filtering deauthentication in wireshark*
 You can use the filter

 ~(wlan.bssid == 00:21:91:d2:8e:25) && !(wlan.fc.type_subtype == 0x08)~

 to monitor all non-Beacon packets to and fro from the access point.

 The && sign stands for the logical AND operator and the ! sign stands for the
 logical NOT operator

**** beating MAC filters

 *finding connected clients to target*
 #+begin_src shell
 $ airodump-ng -c 11 -a --bssid <mac> mon0 
 #+end_src 

 By specifying the *bssid* command, we will only monitor the access point, which is
 of interest to us. The *-c 11* command sets the channel to 11 where the access
 point is. The *-a* command ensures that, in the client section of the airodump-NG
 output, only clients associated and connected to an access point are shown.

 *spoofing MAC address*
 #+begin_src shell
 $ ifconfig wlan0 down
 $ macchanger -m <mac> wlan0
 $ ifconfig wlan0 up
 #+end_src 

*** decrypting WPA packets

#+begin_src shell
# dumping packets with ariodump
$ airodump-ng –bssid 00:21:91:D2:8E:25 --channel 11 --write WPACrackingDemo mon0

# for decrypting WEP packats
$ airdecap-ng -w abcdefabcdefabcdefabcdef12 WEPCrackingDemo-02.cap

# for decrypting WPA packets
$ airdecap-ng –p abdefg WPACrackingDemo-02.cap –e "Wireless Lab"
#+end_src 

*** DOS attacks

** connecting to access point using =iwconfig=

#+begin_src shell
$ iwconfig wlan0 <ESSID>  # ESSID of the target
#+end_src 

** viewing available wireless interfaces

#+begin_src shell
$ iwconfig
#+end_src 

** scan for Access Points

#+begin_src shell 
$ iwcofig wlan0 scan
#+end_src 

* capturing traffic
** concepts
*** ARP
** viewing ARP cache

#+begin_src shell 
$ arp -a
#+end_src 

** ARP cache poisoning

[ubuntu--192.168.20.11] ------------- [ kali-192.168.20.9] ------------------ [winxp - 192.168.20.10]

#+begin_src shell 
# enable IP forwarding
$ echo 1 > /proc/sys/net/ipv4/ip_forward
$ arpspoof -i eth0 -t 192.168.20.11 192.168.20.10
$ arpspoof -i eth0 -t 192.168.20.10 192.168.20.11
#+end_src 

** Using ARP Cache Poisoning to Impersonate the Default Gateway

#+begin_src shell 
root@kali:~# arpspoof -i eth0 -t 192.168.20.11 192.168.20.1
root@kali:~# arpspoof -i eth0 -t 192.168.20.1 192.168.20.11
#+end_src 

* limit CPU usage of a process 

#+begin_src shell 
$ cpulimit -l 50 -p 1234
#+end_src 

Where 1234 is the PID of the process.
* send output from one terminal to another

#+begin_src shell 
$ route > /dev/pts/16
#+end_src 

* apache
** virtual hosts
The basic unit that describes an individual site or domain is called a
virtual host.

These designations allow the administrator to use one server to host
multiple domains or sites off of a single interface or IP by using a
matching mechanism.
** enable =MODE_REWRITE=

#+begin_src shell
$ sudo a2enmod rewrite
#+end_src 

** =RewriteRule=
   
A RewriteRule consists of three arguments separated by spaces. The arguments are
 - Pattern: which incoming URLs should be affected by the rule;
 - Substitution: where should the matching requests be sent;
 - [flags]: options affecting the rewritten request. 

The Substitution can itself be one of three things: 
 - A full filesystem path to a resource
~RewriteRule "^/games" "/usr/local/games/web"~
This maps a request to an arbitrary location on your filesystem, much like the Alias directive. 

 - A web-path to a resource
~RewriteRule "^/foo$" "/bar"~

If DocumentRoot is set to /usr/local/apache2/htdocs, then this directive would
map requests for http://example.com/foo to the path
/usr/local/apache2/htdocs/bar.

 - An absolute URL
~RewriteRule "^/product/view$" "http://site2.example.com/seeproduct.html" [R]~

This tells the client to make a new request for the specified URL. 

The Substitution can also contain back-references to parts of the incoming URL-path matched by the Pattern. Consider the following:
~RewriteRule "^/product/(.*)/view$" "/var/web/productdb/$1"~

** =RewriteCondition=
   
One or more RewriteCond directives can be used to restrict the types of requests
that will be subject to the following RewriteRule.

to send all requests from a particular IP range to a different server, you could
use:

#+begin_src 
RewriteCond "%{REMOTE_ADDR}" "^10\.2\."
RewriteRule "(.*)" "http://intranet.example.com$1" 
#+end_src 

When more than one RewriteCond is specified, they must all match for the
RewriteRule to be applied. For example, to deny requests that contain the word
"hack" in their query string, unless they also contain a cookie containing the
word "go", you could use:

#+begin_src 
RewriteCond "%{QUERY_STRING}" "hack"
RewriteCond "%{HTTP_COOKIE}" "!go"
RewriteRule "." "-" [F] 
#+end_src 

Notice that the exclamation mark specifies a negative match, so the rule is only
applied if the cookie does not contain "go".

Matches in the regular expressions contained in the RewriteConds can be used as
part of the Substitution in the RewriteRule using the variables %1, %2, etc. For
example, this will direct the request to a different directory depending on the
hostname used to access the site:

#+begin_src 
RewriteCond "%{HTTP_HOST}" "(.*)"
RewriteRule "^/(.*)" "/sites/%1/$1" 
#+end_src 

If the request was for http://example.com/foo/bar, then %1 would contain example.com and $1 would contain foo/bar. 

** =mod_access=

#+begin_src
<Directory "/usr/local/apache2/htdocs">
    AllowOverride None
    Order allow,deny
    Allow from all
</Directory>
#+end_src 

 - *Deny,Allow* The Deny directives are evaluated before the Allow directives.
   Access is allowed by default. Any client which does not match a Deny
   directive or does match an Allow directive will be allowed access to the
   server.

 - *Allow,Deny* The Allow directives are evaluated before the Deny directives.
   Access is denied by default. Any client which does not match an Allow
   directive or does match a Deny directive will be denied access to the server.

To block any connection from hosts outside the network 192.168.1.0, you can write:
#+begin_src 
Order Deny,Allow
Deny from all
Allow from 192.168.1.0/24
#+end_src 

To block a particular IP address you can use something like this:

#+begin_src 
Order Allow, Deny
Allow from all
Deny from bad_ip_address_here
#+end_src 

** =.htaccess= for development server

#+begin_src
Order Deny,Allow
Deny from all
Allow from 127.0.0.1
Allow from ::1
#+end_src 

** hot linking protection

#+begin_src 
#Hotlinking Protection
RewriteCond %{HTTP_REFERER} !^$
RewriteCond %{HTTP_REFERER} !^http://(www\.)?subdomain.domain.com/.*$ [NC]
RewriteRule \.(js|css|jpg|gif|png|bmp|mp4|3gp|m4a|m4r|aac|mp3|ogg|wave)$ - [F]
#+end_src 

** Starting, Stopping, and Restarting Apache

apachectl start

This will start the server if it isn't already running. If it is running, this
option has no effect and may produce a warning message.

apachectl graceful

This option causes the server to reload its configuration files and gracefully
restart its operation. Any current connections in progress are allowed to
complete. The server will be started if it isn't running.

apachectl restart

Like the graceful option, this one makes the server reload its configuration
files. However, existing connections are terminated immediately. If the server
isn't running, this command will try to start it.

apachectl stop

This shuts the server down immediately. Any existing connections are terminated at once.

** What are Entity Tags?
   
Entity tags, also commonly referred to as ETags, are cache validators which help
the browser determine if it can retrieve the requested resource from local cache
or if it must be retrieved from the server. This mechanism helps improve loading
times since if the resource can be retrieved from local cache, the browser does
not need to make an additional request to the server.

A traditional ETag is comprised of three separate components which make it an
unique identifier for each resource:

 - INode
 - MTime
 - Size

An example of what an ETag may resemble containing all three components would be
similar to 13630e1-b438-524daace96280. However, this may change in structure
depending upon the web server, if the ETag is using strong or weak validation,
and if you configure entity tags (ETags).

** Setting the Default Filename for a Directory or Entire Site
You need to tell your web server the name of the default page for a given
directory or all directories on your web site.

Add or modify the DirectoryIndex enTRy in your httpd.conf file, or a specific direc-tory's .htaccess file. List the files that should be treated as default pages in the order you wish them to be served: 
#+begin_src
	DirectoryIndex index.php index.html index.htm index.php3 welcome.html
#+end_src 

** Making Sure Your Web Site Loads With and Without the "www" Prefix

If your web server has the rewrite module enabled, you can create rules that
tell Apache to seamlessly change the URL requested by the browser to something
else. For example, requests for http://www.domain.com become http://domain.com.
To do this, create or modify the .htaccess file in your web root directory with
a rewrite rule to remove the "www." from browser requests to your web site.
First, find or create an .htaccess file. Then copy into it the code shown below,
replacing domain.com with your domain name:

#+begin_src 
	RewriteEngine On
	RewriteCond %{HTTP_HOST} ^www\.domain\.com$ [NC]
	RewriteRule ^(.*)$ http://domain.com/$1 [R=301,L]
#+end_src 

Create a directory at the top level of your hosting account home directoryin
other words, at the same level as your current home page HTML file. Call the
directory something obvious, such as www, web, or htdocs. I'm going to use
htdocs for the example below. Now create or modify the .htaccess file in your
home directory. Copy into it the following rules for redirecting requests to
your domain name to files in the htdocs directory:

#+begin_src 
	RewriteEngine on
	RewriteCond $1 !^htdocs/
	RewriteRule (.*) / htdocs /$1 [L]
	RewriteCond %{THE_REQUEST} ^[A-Z]+\ / htdocs /
	RewriteRule .* - [F]
#+end_src 

The first line ensures that the Apache rewrite engine is on. Lines two and three
invisibly redirect browser requests to files in the htdocs directory, but keep
the rule from looping indefinitely. Because the rules in the .htaccess file
apply to the directory the file is in as well as all the directories below
itincluding our new htdocs directoryline 3 prevents Apache from appending an
infinite number of htdocs to the browser request. Lines 4 and 5 prevent direct
requests for files in the htdocs directory.

** enable ~gzip~ compression via ~.htaccess~
#+begin_src
<ifModule mod_gzip.c>
mod_gzip_on Yes
mod_gzip_dechunk Yes
mod_gzip_item_include file .(html?|txt|css|js|php|pl)$
mod_gzip_item_include handler ^cgi-script$
mod_gzip_item_include mime ^text/.*
mod_gzip_item_include mime ^application/x-javascript.*
mod_gzip_item_exclude mime ^image/.*
mod_gzip_item_exclude rspheader ^Content-Encoding:.*gzip.*
</ifModule>
#+end_src 
Enable compression on Apache webservers The instructions and code above will
work on Apache. If they are not working there is another way that may work for
you. If the above code did not seem to work, remove it from your .htaccess file
and try this one instead...

#+begin_src 
AddOutputFilterByType DEFLATE text/plain
AddOutputFilterByType DEFLATE text/html
AddOutputFilterByType DEFLATE text/xml
AddOutputFilterByType DEFLATE text/css
AddOutputFilterByType DEFLATE application/xml
AddOutputFilterByType DEFLATE application/xhtml+xml
AddOutputFilterByType DEFLATE application/rss+xml
AddOutputFilterByType DEFLATE application/javascript
AddOutputFilterByType DEFLATE application/x-javascript
#+end_src 

https://varvy.com/pagespeed/enable-compression.html

* redis
** using from command line

#+begin_src
$ redis-cli
127.0.0.1:6379> SET philosopher "socrates"
OK
127.0.0.1:6379> GET philosopher
"socrates"
127.0.0.1:6379>
#+end_src 

** Redis data types
*** Strings
**** intro
In Redis, STRING s are used to store three types of values:

 - Byte string values
 - Integer values
 - Floating-point values

**** MSET and MGET commands
The *MSET* command sets the values of multiple keys at once. The arguments are
key-value pairs separated by spaces.
The *MGET* command retrieves the values of multiple key names at once, and the
key names are separated by spaces.

#+begin_src 
$ redis-cli
127.0.0.1:6379> MSET first "First Key value" second "Second Key value"
OK
127.0.0.1:6379> MGET first second
1) "First Key value"
2) "Second Key value"
#+end_src

**** =INCR= and =DECR= 
     
*Increment and decrement commands in Redis*
| Command     | Example use and description                                                                                                        |
|-------------+------------------------------------------------------------------------------------------------------------------------------------|
| INCR        | INCR key-name —Increments the value stored at the key by 1                                                                         |
| DECR        | DECR key-name —Decrements the value stored at the key by 1                                                                         |
| INCRBY      | INCRBY key-name amount —Increments the value stored at the key by the provided integer value                                       |
| DECRBY      | DECRBY key-name amount —Decrements the value stored at the key by the provided integer value                                       |
| INCRBYFLOAT | INCRBYFLOAT key-name amount —Increments the value stored at the key by the provided float value (available in Redis 2.6 and later) |

***** python example

#+begin_src
>>> conn = redis.Redis()
>>> conn.get('key')
>>> conn.incr('key')
1
>>> conn.incr('key', 15)
16
>>> conn.decr('key', 5)
11
>>> conn.get('key')
'11'
>>> conn.set('key', '13')
True
>>> conn.incr('key')
14
#+end_src

**** Substring manipulation

*Substring manipulation commands available to Redis*
| Command  | Example use and description                                                                                                                                                      |
|----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| APPEND   | APPEND key-name value —Concatenates the provided value to the string already stored at the given key                                                                             |
| GETRANGE | GETRANGE key-name start end —Fetches the substring including all characters from the start offset to the end offset inclusive.                                                   |
| SETRANGE | SETRANGE key-name offset value —Sets the substring starting at the pro- vided offset to the given value.                                                                         |
| GETBIT   | GETBIT key-name offset —Treats the byte string as a bit string and returns the value of the bit in the string at the provided bit offset.                                        |
| SETBIT   | SETBIT key-name offset value —Treats the byte string as a bit string and sets the value of the bit in the string at the provided bit offset.                                     |
| BITCOUNT | BITCOUNT key-name [start end] —Counts the number of 1 bits in the string optionally starting and finishing at the provided byte offsets                                          |
| BITOP    | BITOP operation dest-key key-name [key-name ...] —Performs one of the bitwise operations AND  OR  XOR  or NOT  on the strings provided storing the result in the destination key |

***** python example

#+begin_src 
>>> conn.append('new-string-key', 'hello ')
6L
>>> conn.append('new-string-key', 'world!')
12L
>>> conn.substr('new-string-key', 3, 7)
'lo wo'
>>> conn.setrange('new-string-key', 0, 'H')
12
>>> conn.setrange('new-string-key', 6, 'W')
12
>>> conn.get('new-string-key')
'Hello World!'
>>> conn.setrange('new-string-key', 11, ', how are you?')
25
>>> conn.get('new-string-key')
'Hello World, how are you?'
>>> conn.setbit('another-key', 2, 1)
0
>>> conn.setbit('another-key', 7, 1)
0
>>> conn.get('another-key')
'!'  # We set bits 2 and 7 to 1, which gave us ‘!’, or character 33.
#+end_src

*** =EXPIRE= command
The EXPIRE command adds an expiration time (in seconds) to a given key. After that
time, the key is automatically deleted. It returns 1 if the expiration is set successfully
and 0 if the key does not exist or cannot be set.
The TTL (Time To Live) command returns one of the following:
•	 A positive integer: This is the amount of seconds a given key
has left to live
•	 -2: If the key is expired or does not exist
•	 -1: If the key exists but has no expiration time set

#+begin_src 
$ redis-cli
127.0.0.1:6379> SET current_chapter "Chapter 1"
OK
127.0.0.1:6379> EXPIRE current_chapter 10
(integer) 1
127.0.0.1:6379> GET current_chapter
"Chapter 1"
127.0.0.1:6379> TTL current_chapter
(integer) 3
127.0.0.1:6379> TTL current_chapter
(integer) -2
127.0.0.1:6379> GET current_chapter
(nil)
127.0.0.1:6379>
#+end_src 

The command INCRBYFLOAT increments a key by a given float number and returns
the new value. INCRBY, DECRBY, and INCRBYFLOAT accept either a positive or a
negative number:

#+begin_src 
$ redis-cli
127.0.0.1:6379> SET counter 100
OK
127.0.0.1:6379> INCR counter
(integer) 101
127.0.0.1:6379> INCRBY counter 5
(integer) 106
127.0.0.1:6379> DECR counter
(integer) 105
127.0.0.1:6379> DECRBY counter 100
(integer) 5
127.0.0.1:6379> GET counter
"5"
127.0.0.1:6379> INCRBYFLOAT counter 2.4
"7.4"
#+end_src 

The preceding commands shown are atomic, which means that they increment/
decrement and return the new value as a single operation. It is not possible for two
different clients to execute the same command at the same time and get the same
result—no race conditions happen with those commands.
For example, if the counter key is 1 and two different clients (A and B) increment
their counters at the same time with INCR, client A will receive the value 2 and
client B will receive 3.

*** Lists
**** intro
List commands are atomic. There are blocking commands in
Redis's Lists, which means that when a client executes a blocking command in an
empty List, the client will wait for a new item to be added to the List.

Redis's Lists are linked lists, therefore insertions and deletions from the beginning or the end
of a List run in O(1), constant time.

The task of accessing an element in a List runs in O(N), linear time, but accessing
the first or last element always runs in constant time.

**** List examples with redis-cli

The command *LPUSH* inserts data at the beginning of a List
(left push), and the command *RPUSH* inserts data at the end of a List (right push):

#+begin_src 
$ redis-cli
127.0.0.1:6379> LPUSH books "Clean Code"
(integer) 1
127.0.0.1:6379> RPUSH books "Code Complete"
(integer) 2
127.0.0.1:6379> LPUSH books "Peopleware"
(integer) 3
#+end_src 

The command *LLEN* returns the length of a List. The command *LINDEX* returns
the element in a given index (indices are zero-based). Elements in a List are always
accessed from left to right, which means that index 0 is the first element, index 1
is the second element, and so on. It is possible to use negative indices to access the
tail of the List, in which -1 is the last element, -2 is penultimate element, and so on.
LINDEX does not modify a List:

#+begin_src 
$ redis-cli
127.0.0.1:6379> LLEN books
(integer) 3
127.0.0.1:6379> LINDEX books 1
"Clean Code"
#+end_src 

The command *LRANGE* returns an array with all elements from a given index range,
including the elements in both the start and end indices. As we mentioned previously,
indices are zero-based and can be positive or negative. See the following example:

#+begin_src 
$ redis-cli
127.0.0.1:6379> LRANGE books 0 1
1) "Peopleware"
2) "Clean Code"
127.0.0.1:6379> LRANGE books 0 -1
1) "Peopleware"
2) "Clean Code"
3) "Code Complete"
#+end_src 

The command *LPOP* removes and returns the first element of a List. The command
*RPOP* removes and returns the last element of a List. Unlike LINDEX, both LPOP
and RPOP modify the List:

#+begin_src 
$ redis-cli
127.0.0.1:6379> LPOP books
"Peopleware"
127.0.0.1:6379> RPOP books
"Code Complete"
127.0.0.1:6379> LRANGE books 0 -1
1) "Clean Code"
#+end_src 

**** python example

#+begin_src
>>> conn.rpush('list-key', 'last')          #A
1L                                          #A
>>> conn.lpush('list-key', 'first')         #B
2L
>>> conn.rpush('list-key', 'new last')
3L
>>> conn.lrange('list-key', 0, -1)          #C
['first', 'last', 'new last']               #C
>>> conn.lpop('list-key')                   #D
'first'                                     #D
>>> conn.lpop('list-key')                   #D
'last'                                      #D
>>> conn.lrange('list-key', 0, -1)
['new last']
>>> conn.rpush('list-key', 'a', 'b', 'c')   #E
4L
>>> conn.lrange('list-key', 0, -1)
['new last', 'a', 'b', 'c']
>>> conn.ltrim('list-key', 2, -1)           #F
True                                        #F
>>> conn.lrange('list-key', 0, -1)          #F
['b', 'c']                                  #F
#A When we push items onto the list, it returns the length of the list after the push has completed
#B We can easily push on both ends of the list
#C Semantically, the left end of the list is the beginning, and the right end of the list is the end
#D Popping off the left items repeatedly will return items from left to right
#E We can push multiple items at the same time
#F We can trim any number of items from the start, end, or both
#+end_src

**** python List blocking example 

#+begin_src
>>> conn.rpush('list', 'item1')             #A
1                                           #A
>>> conn.rpush('list', 'item2')             #A
2                                           #A
>>> conn.rpush('list2', 'item3')            #A
1                                           #A
>>> conn.brpoplpush('list2', 'list', 1)     #B
'item3'                                     #B
>>> conn.brpoplpush('list2', 'list', 1)     #C
>>> conn.lrange('list', 0, -1)              #D
['item3', 'item1', 'item2']                 #D
>>> conn.brpoplpush('list', 'list2', 1)
'item2'
>>> conn.blpop(['list', 'list2'], 1)        #E
('list', 'item3')                           #E
>>> conn.blpop(['list', 'list2'], 1)        #E
('list', 'item1')                           #E
>>> conn.blpop(['list', 'list2'], 1)        #E
('list2', 'item2')                          #E
>>> conn.blpop(['list', 'list2'], 1)        #E
#A Let's add some items to a couple lists to start
#B Let's move an item from one list to the other, also returning the item
#C When a list is empty, the blocking pop will stall for the timeout, and return None (which isn't displayed in the interactive console)
#D We popped the rightmost item from 'list2' and pushed it to the left of 'list'
#E Blocking left-popping items from these will check lists for items in the order that they are passed, until they are empty
#+end_src 

*** Hashes
**** intro
Hashes are a great data structure for storing objects because you can map fields to
values. They are optimized to use memory efficiently and look for data very fast.
In a Hash, both the field name and the value are Strings. Therefore, a Hash is a
mapping of a String to a String.

**** Using Hashes with redis-cli

The command HSET sets a value to a field of a given key. The syntax is HSET key
field value.
The command HMSET sets multiple field values to a key, separated by spaces.
Both HSET and HMSET create a field if it does not exist, or overwrite its value
if it already exists.

The command HINCRBY increments a field by a given integer. Both HINCRBY
and HINCRBYFLOAT are similar to INCRBY and INCRBYFLOAT (not presented
in the following code):

#+begin_src 
$ redis-cli
127.0.0.1:6379> HSET movie "title" "The Godfather"
(integer) 1
127.0.0.1:6379> HMSET movie "year" 1972 "rating" 9.2 "watchers" 10000000
OK
127.0.0.1:6379> HINCRBY movie "watchers" 3
(integer) 10000003
#+end_src 

The command HGET retrieves a field from a Hash. The command HMGET retrieves
multiple fields at once:

#+begin_src 
127.0.0.1:6379> HGET movie "title"
"The Godfather"
127.0.0.1:6379> HMGET movie "title" "watchers"
1) "The Godfather"
2) "10000003"
The command HDEL deletes a field from a Hash:
127.0.0.1:6379> HDEL movie "watchers"
(integer) 1   

#+end_src 

The command HGETALL returns an array of all field/value pairs in a Hash:

#+begin_src 
127.0.0.1:6379> HGETALL movie
1) "title"
2) "The Godfather"
3) "year"
4) "1972"
5) "rating"
6) "9.2"
127.0.0.1:6379>
#+end_src 

It is possible to retrieve only the field names or field values of a Hash with the
commands HKEYS and HVALS respectively.

**** python example

#+begin_src 
>>> conn.hmset('hash-key', {'k1':'v1', 'k2':'v2', 'k3':'v3'})   #A
True                                                            #A
>>> conn.hmget('hash-key', ['k2', 'k3'])                        #B
['v2', 'v3']                                                    #B
>>> conn.hlen('hash-key')                                       #C
3                                                               #C
>>> conn.hdel('hash-key', 'k1', 'k3')                           #D
True                                                            #D

#A We can add multiple items to the hash in one call
#B We can fetch a subset of the values in a single call
#C The HLEN command is typically used for debugging very large HASHes
#D The HDEL command handles multiple arguments without needing an HMDEL counterpart and returns True if any fields were removed
#+end_src

**** python advanced Hash functions example 

#+begin_src 
>>> conn.hmset('hash-key2', {'short':'hello', 'long':1000*'1'}) #A
True                                                            #A
>>> conn.hkeys('hash-key2')                                     #A
['long', 'short']                                               #A
>>> conn.hexists('hash-key2', 'num')                            #B
False                                                           #B
>>> conn.hincrby('hash-key2', 'num')                            #C
1L                                                              #C
>>> conn.hexists('hash-key2', 'num')                            #C
True                                                            #C
#A Fetching keys can be useful to keep from needing to transfer large values when you are looking into HASHes
#B We can also check the existence of specific keys
#C Incrementing a previously non-existent key in a hash behaves just like on strings, Redis operates as though the value had been 0
#+end_src 

*** sets 
**** intro
A Set in Redis is an unordered collection of distinct Strings—it's not possible to add
repeated elements to a Set. Internally, a Set is implemented as a hash table, which
is the reason that some operations are optimized: member addition, removal, and
lookup run in O(1), constant time.

**** Set examples with redis-cli
The command *SADD* is responsible for adding one or many members to a Set. SADD
ignores members that already exist in a Set and returns the number of added members:

#+begin_src 
$ redis-cli
127.0.0.1:6379> SADD user:max:favorite_artists "Arcade Fire" "Arctic Monkeys"
"Belle & Sebastian" "Lenine"
(integer) 4
127.0.0.1:6379> SADD user:hugo:favorite_artists "Daft Punk" "The Kooks" "Arctic
Monkeys"
(integer) 3
#+end_src 

The command SINTER expects one or many Sets and returns an array with the
members that belong to every Set. In this example, SINTER returns only the favorite
artists that both Max and Hugo have on their lists:

#+begin_src 
127.0.0.1:6379> SINTER user:max:favorite_artists user:hugo:favorite_artists
1) "Arctic Monkeys"
#+end_src 

The command SDIFF expects one or many Sets. It returns an array with all members
of the first Set that do not exist in the Sets that follow it. In this command, the key
name order matters. Any key that does not exist is considered to be an empty Set.
There are two ways of using the command SDIFF.

The first example returns the names of artists from user:max:favorite_artists that are
not present in user:hugo:favorite_artists:

#+begin_src 
127.0.0.1:6379> SDIFF user:max:favorite_artists user:hugo:favorite_artists
1) "Belle & Sebastian"
2) "Arcade Fire"
3) "Lenine"
#+end_src

The second example returns the names of artists from user:hugo:favorite_artists that
are not present in user:max:favorite_artists:
#+begin_src 
127.0.0.1:6379> SDIFF user:hugo:favorite_artists user:max:favorite_artists
1) "Daft Punk"
2) "The Kooks"
#+end_src 

The SUNION command expects one or many Sets. It returns an array with all
members of all Sets. The result has no repeated members.
In this example, SUNION returns the names of all artists in both users' Sets of
favorite artists:

#+begin_src 
127.0.0.1:6379> SUNION user:max:favorite_artists user:hugo:favorite_artists
1) "Lenine"
2) "Daft Punk"
3) "Belle & Sebastian"
4) "Arctic Monkeys"
5) "Arcade Fire"
6) "The Kooks"
#+end_src 

The command SRANDMEMBER returns random members from a Set. Because Sets
are unordered, it is not possible to retrieve elements from a given position:
#+begin_src 
127.0.0.1:6379> SRANDMEMBER user:max:favorite_artists
"Arcade Fire"
127.0.0.1:6379> SRANDMEMBER user:max:favorite_artists
"Lenine"
#+end_src 

The command SISMEMBER checks whether a member exists in a Set. It returns 1
if the member exists and 0 if it does not.
The command SREM removes and returns members from a Set. The command
SCARD returns the number of members in a Set (also known as cardinality):
#+begin_src 
127.0.0.1:6379> SISMEMBER user:max:favorite_artists "Arctic Monkeys"
(integer) 1
127.0.0.1:6379> SREM user:max:favorite_artists "Arctic Monkeys"
(integer) 1
127.0.0.1:6379> SISMEMBER user:max:favorite_artists "Arctic Monkeys"
(integer) 0
127.0.0.1:6379> SCARD user:max:favorite_artists
(integer) 3
#+end_src 

The command *SMEMBERS* returns an array with all members of a Set:

#+begin_src
127.0.0.1:6379> SMEMBERS user:max:favorite_artists
1) "Belle & Sebastian"
2) "Arcade Fire"
3) "Lenine"
#+end_src

**** python example 

#+begin_src
>>> conn.sadd('set-key', 'a', 'b', 'c')         #A
3                                               #A
>>> conn.srem('set-key', 'c', 'd')              #B
True                                            #B
>>> conn.srem('set-key', 'c', 'd')              #B
False                                           #B
>>> conn.scard('set-key')                       #C
2                                               #C
>>> conn.smembers('set-key')                    #D
set(['a', 'b'])                                 #D
>>> conn.smove('set-key', 'set-key2', 'a')      #E
True                                            #E
>>> conn.smove('set-key', 'set-key2', 'c')      #F
False                                           #F
>>> conn.smembers('set-key2')                   #F
set(['a'])                                      #F
#A Adding items to the SET returns the number of items that weren't already in the SET
#B Removing items from the SET returns whether an item was removed - note that the client is buggy in that respect, as Redis itself returns the total number of items removed
#C We can get the number of items in the SET
#D We can also fetch the whole SET
#E We can easily move items from one SET to another SET
#F When an item doesn't exist in the first set during a SMOVE, it isn't added to the destination SET
#+end_src

**** SET difference, intersection, and union in Redis python example

#+begin_src
>>> conn.sadd('skey1', 'a', 'b', 'c', 'd')  #A
4                                           #A
>>> conn.sadd('skey2', 'c', 'd', 'e', 'f')  #A
4                                           #A
>>> conn.sdiff('skey1', 'skey2')            #B
set(['a', 'b'])                             #B
>>> conn.sinter('skey1', 'skey2')           #C
set(['c', 'd'])                             #C
>>> conn.sunion('skey1', 'skey2')           #D
set(['a', 'c', 'b', 'e', 'd', 'f'])         #D
#A First we'll add a few items to a couple SETs
#B We can calculate the result of removing all of the items in the second set from the first SET
#C We can also find out which items exist in both SETs
#D And we can find out all of the items that are in either of the SETs
#+end_src

*** sorted sets
**** intro

ZSET s offer the ability to store a mapping of members to scores (similar to the keys and
values of HASH es). These mappings allow us to manipulate the numeric scores, and
fetch and scan over both members and scores based on the sorted order of the scores.

Some common ZSET commands
| Command | Example use and description                                                                                                              |
|---------+------------------------------------------------------------------------------------------------------------------------------------------|
| ZADD    | ZADD key-name score member [score member ...] —Adds members with the given scores to the ZSET                                            |
| ZREM    | ZREM key-name member [member ...] —Removes the members from the ZSET  returning the number of members that were removed                  |
| ZCARD   | ZCARD key-name —Returns the number of members in the ZSET ZINCRBY ZINCRBY key-name increment member —Increments the member in the ZSET   |
| ZCOUNT  | ZCOUNT key-name min max —Returns the number of members with scores between the provided minimum and maximum                              |
| ZRANK   | ZRANK key-name member —Returns the position of the given member in the ZSET                                                              |
| ZSCORE  | ZSCORE key-name member —Returns the score of the member in the ZSET                                                                      |
| ZRANGE  | ZRANGE key-name start stop [WITHSCORES] —Returns the members and optionally the scores for the members with ranks between start and stop |


Commands for fetching and deleting ranges of data from ZSET s and offering SET -like intersections Command
| Example          | use and description                                                                                                                                          |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ZREVRANK         | ZREVRANK key-name member —Returns the position of the member in the ZSET  with members ordered in reverse                                                    |
| ZREVRANGE        | ZREVRANGE key-name start stop [WITHSCORES] —Fetches the given members from the ZSET by rank with members in reverse order                                    |
| ZRANGEBYSCORE    | ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] —Fetches the members between min and max                                                         |
| ZREVRANGEBYSCORE | ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] —Fetches the members in reverse order between min and max                                     |
| ZREMRANGEBYRANK  | ZREMRANGEBYRANK key-name start stop —Removes the items from the ZSET with ranks between start and stop                                                       |
| ZREMRANGEBYSCORE | ZREMRANGEBYSCORE key-name min max —Removes the items from the ZSET with scores between min and max                                                           |
| ZINTERSTORE      | ZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM,MIN,MAX] —Performs a SET -like intersection of the provided ZSET s |
| ZUNIONSTORE      | ZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM,MIN,MAX] —Performs a SET -like union of the provided ZSET s        |

**** python example

#+begin_src
>>> conn.zadd('zset-key', 'a', 3, 'b', 2, 'c', 1)   #A
3                                                   #A
>>> conn.zcard('zset-key')                          #B
3                                                   #B
>>> conn.zincrby('zset-key', 'c', 3)                #C
4.0                                                 #C
>>> conn.zscore('zset-key', 'b')                    #D
2.0                                                 #D
>>> conn.zrank('zset-key', 'c')                     #E
2                                                   #E
>>> conn.zcount('zset-key', 0, 3)                   #F
2L                                                  #F
>>> conn.zrem('zset-key', 'b')                      #G
True                                                #G
>>> conn.zrange('zset-key', 0, -1, withscores=True) #H
[('a', 3.0), ('c', 4.0)]                            #H
#A Adding members to ZSETs in Python has the arguments reversed compared to standard Redis, so as to not confuse users compared to HASHes
#B Knowing how large a ZSET is can tell you in some cases if it is necessary to trim your ZSET
#C We can also increment members like we can with STRING and HASH values
#D Fetching scores of individual members can be useful if you have been keeping counters or toplists
#E By fetching the 0-indexed position of a member, we can then later use ZRANGE to fetch a range of the values easily
#F Counting the number of items with a given range of scores can be quite useful for some tasks
#G Removing members is as easy as adding them
#H For debugging, we usually fetch the entire ZSET with this ZRANGE call, but real use-cases will usually fetch items a relatively small group at a time
#+end_src

**** python =ZINTERSTORE= and =ZUNIONSTORE= example

#+begin_src 
>>> conn.zadd('zset-1', 'a', 1, 'b', 2, 'c', 3)                         #A
3                                                                       #A
>>> conn.zadd('zset-2', 'b', 4, 'c', 1, 'd', 0)                         #A
3                                                                       #A
>>> conn.zinterstore('zset-i', ['zset-1', 'zset-2'])                    #B
2L                                                                      #B
>>> conn.zrange('zset-i', 0, -1, withscores=True)                       #B
[('c', 4.0), ('b', 6.0)]                                                #B
>>> conn.zunionstore('zset-u', ['zset-1', 'zset-2'], aggregate='min')   #C
4L                                                                      #C
>>> conn.zrange('zset-u', 0, -1, withscores=True)                       #C
[('d', 0.0), ('a', 1.0), ('c', 1.0), ('b', 2.0)]                        #C
>>> conn.sadd('set-1', 'a', 'd')                                        #D
2                                                                       #D
>>> conn.zunionstore('zset-u2', ['zset-1', 'zset-2', 'set-1'])          #D
4L                                                                      #D
>>> conn.zrange('zset-u2', 0, -1, withscores=True)                      #D
[('d', 1.0), ('a', 2.0), ('c', 4.0), ('b', 6.0)]                        #D

#A We'll start out by creating a couple ZSETs
#B When performing ZINTERSTORE or ZUNIONSTORE, our default aggregate is sum, so scores of items that are in multiple ZSETs are added
#C It is easy to provide different aggregates, though we are limited to sum, min, and max
#D You can also pass SETs as inputs to ZINTERSTORE and ZUNIONSTORE, they behave as though they were ZSETs with all scores equal to 1
#+end_src

*** Bitmaps
**** intro    
A Bitmap is not a real data type in Redis. Under the hood, a Bitmap is a String.
We can also say that a Bitmap is a set of bit operations on a String.
A Bitmap is a sequence of bits where each bit can store 0 or 1. You can think of a
Bitmap as an array of ones and zeroes.

****  Bitmap examples with redis-cli
The SETBIT command is used to give a value to a Bitmap offset, and it accepts only
1 or 0. If the Bitmap does not exist, it creates it. In the following snippet, users 10 and
15 visited the website on 2015-01-01, and users 10 and 11 visited it on 2015-01-02:

#+begin_src 
127.0.0.1:6379> SETBIT visits:2015-01-01 10 1
(integer) 0
127.0.0.1:6379> SETBIT visits:2015-01-01 15 1
(integer) 0
127.0.0.1:6379> SETBIT visits:2015-01-02 10 1
(integer) 0
127.0.0.1:6379> SETBIT visits:2015-01-02 11 1
(integer) 0
#+end_src 

The GETBIT command returns the value of a Bitmap offset. In the following example,
it checks whether user 10 visited the website on 2015-01-01, and then checks whether
user 15 visited the website on 2015-01-02:

#+begin_src 
127.0.0.1:6379> GETBIT visits:2015-01-01 10
(integer) 1
127.0.0.1:6379> GETBIT visits:2015-01-02 15
(integer) 0
#+end_src 

The BITCOUNT command returns the number of bits marked as 1 in a Bitmap.
In this example, it returns the number of users who visited the website on the
specified dates:

#+begin_src 
127.0.0.1:6379> BITCOUNT visits:2015-01-01
(integer) 2
127.0.0.1:6379> BITCOUNT visits:2015-01-02
(integer) 2
#+end_src 

The BITOP command requires a destination key, a bitwise operation, and a list
of keys to apply to that operation and store the result in the destination key. The
available bitwise operations are OR, AND, XOR, and NOT. The following example
uses BITOP OR to find out how many users visited the website on the specified
dates (2015-01-01 and 2015-01-02):

#+begin_src 
127.0.0.1:6379> BITOP OR total_users visits:2015-01-01 visits:2015-01-02
(integer) 2
127.0.0.1:6379> BITCOUNT total_users
(integer) 3
#+end_src

** publish/subscribe
*** intro
Generally,the concept of publish/ subscribe, also known as pub/sub, is
characterized by listeners subscribing to channels, with publishers sending
binary string messages to channels. Anyone listening to a given channel will
receive all messages sent to that channel while they’re connected and lis-
tening. You can think of it like a radio station, where subscribers can listen
to multiple radio stations at the same time, and publishers can send messages on
any radio station.

Commands for handling pub/sub in Redis

| Command      | Example use and description                                                                                                                  |
|--------------+----------------------------------------------------------------------------------------------------------------------------------------------|
| SUBSCRIBE    | SUBSCRIBE channel [channel ...] —Subscribes to the given channels                                                                            |
| UNSUBSCRIBE  | UNSUBSCRIBE [channel [channel ...]] —Unsubscribes from the provided channels or unsubscribes all channels if no channel is given             |
| PUBLISH      | PUBLISH channel message —Publishes a message to the given channel                                                                            |
| PSUBSCRIBE   | PSUBSCRIBE pattern [pattern ...] —Subscribes to messages broadcast to channels that match the given pattern                                  |
| PUNSUBSCRIBE | PUNSUBSCRIBE [pattern [pattern ...]] —Unsubscribes from the provided patterns or unsubscribes from all subscribed patterns if none are given |

*** python example
#+begin_src 
>>> def publisher(n):
...     time.sleep(1)                                                   #A
...     for i in xrange(n):
...         conn.publish('channel', i)                                  #B
...         time.sleep(1)                                               #B
...
>>> def run_pubsub():
...     threading.Thread(target=publisher, args=(3,)).start()           #D
...     pubsub = conn.pubsub()                                          #E
...     pubsub.subscribe(['channel'])                                   #E
...     count = 0
...     for item in pubsub.listen():                                    #F
...         print item                                                  #G
...         count += 1                                                  #H
...         if count == 4:                                              #H
...             pubsub.unsubscribe()                                    #H
...         if count == 5:                                              #L
...             break                                                   #L
...
>>> run_pubsub()                                                        #C
{'pattern': None, 'type': 'subscribe', 'channel': 'channel', 'data': 1L}#I
{'pattern': None, 'type': 'message', 'channel': 'channel', 'data': '0'} #J
{'pattern': None, 'type': 'message', 'channel': 'channel', 'data': '1'} #J
{'pattern': None, 'type': 'message', 'channel': 'channel', 'data': '2'} #J
{'pattern': None, 'type': 'unsubscribe', 'channel': 'channel', 'data':  #K
0L}                                                                     #K
#A We sleep initially in the function to let the SUBSCRIBEr connect and start listening for messages
#B After publishing, we will pause for a moment so that we can see this happen over time
#D Let's start the publisher thread to send 3 messages
#E We'll set up the pubsub object and subscribe to a channel
#F We can listen to subscription messages by iterating over the result of pubsub.listen()
#G We'll print every message that we receive
#H We will stop listening for new messages after the subscribe message and 3 real messages by unsubscribing
#L When we receive the unsubscribe message, we need to stop receiving messages
#C Actually run the functions to see them work
#I When subscribing, we receive a message on the listen channel
#J These are the structures that are produced as items when we iterate over pubsub.listen()
#K When we unsubscribe, we receive a message telling us which channels we have unsubscribed from and the number of channels we are still subscribed to
#+end_src

** sorting
*** intro

SORT allows us to sort LIST s, SET s, and ZSET s according to data in the LIST /
SET / ZSET data stored in STRING keys, or even data stored in HASH es. If you’re
coming from a relational database background, you can think of SORT as like the
order by clause in a SQL statement that can reference other rows and tables.

*The SORT command definition*
Command Example use and description
 - SORT 
 SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern
 ...]] [ASC|DESC] [ALPHA] [STORE dest-key] —Sorts the input LIST , SET , or ZSET
 according to the options provided, and returns or stores the result

*** python example
#+begin_src
>>> conn.rpush('sort-input', 23, 15, 110, 7)                    #A
4                                                               #A
>>> conn.sort('sort-input')                                     #B
['7', '15', '23', '110']                                        #B
>>> conn.sort('sort-input', alpha=True)                         #C
['110', '15', '23', '7']                                        #C
>>> conn.hset('d-7', 'field', 5)                                #D
1L                                                              #D
>>> conn.hset('d-15', 'field', 1)                               #D
1L                                                              #D
>>> conn.hset('d-23', 'field', 9)                               #D
1L                                                              #D
>>> conn.hset('d-110', 'field', 3)                              #D
1L                                                              #D
>>> conn.sort('sort-input', by='d-*->field')                    #E
['15', '110', '7', '23']                                        #E
>>> conn.sort('sort-input', by='d-*->field', get='d-*->field')  #F
['1', '3', '5', '9']                                            #F

#A Start by adding some items to a LIST
#B We can sort the items numerically
#C And we can sort the items alphabetically
#D We are just adding some additional data for SORTing and fetching
#E We can sort our data by fields of HASHes
#F And we can even fetch that data and return it instead of or in addition to our input data
#+end_src

Sorting can be used to sort LIST s, but it can also sort SET s, turning the result into a
LIST . In this example, we sorted numbers character by character (via the alpha key-
word argument), we sorted some items based on external data, and we were even able
to fetch external data to return.

** transactions
*** basic Redis transactions
In Redis, a basic transaction involving MULTI and EXEC is meant to provide the oppor-
tunity for one client to execute multiple commands A, B, C, ... without other clients
being able to interrupt them. This isn’t the same as a relational database transaction,
which can be executed partially, and then rolled back or committed. In Redis, every
command passed as part of a basic MULTI / EXEC transaction is executed one after
another until they’ve completed. After they’ve completed, other clients may execute
their commands.

To perform a transaction in Redis, we first call MULTI , followed by any sequence
of commands we intend to execute, followed by EXEC . When seeing MULTI , Redis will
queue up commands from that same connection until it sees an EXEC , at which point
Redis will execute the queued commands sequentially without interruption. Seman-
tically, our Python library handles this by the use of what’s called a pipeline. Calling
the pipeline() method on a connection object will create a transaction, which
when used correctly will automatically wrap a sequence of commands with MULTI
and EXEC . Incidentally, the Python Redis client will also store the commands to send
until we actually want to send them. This reduces the number of round trips
between Redis and the client, which can improve the performance of a sequence
of commands.

*** python example

#+begin_src 
>>> def notrans():
...     print conn.incr('notrans:')                     #A
...     time.sleep(.1)                                  #B
...     conn.incr('notrans:', -1)                       #C
...
>>> if 1:
...     for i in xrange(3):                             #D
...         threading.Thread(target=notrans).start()    #D
...     time.sleep(.5)                                  #E
...
1                                                       #F
2                                                       #F
3                                                       #F
# <end id="simple-pipeline-notrans"/>
#A Increment the 'notrans:' counter and print the result
#B Wait for 100 milliseconds
#C Decrement the 'notrans:' counter
#D Start three threads to execute the non-transactional increment/sleep/decrement
#E Wait half a second for everything to be done
#F Because there is no transaction, each of the threaded commands can interleave freely, causing the counter to steadily grow in this case
#END
'''

'''
>>> def trans():
...     pipeline = conn.pipeline()                      #A
...     pipeline.incr('trans:')                         #B
...     time.sleep(.1)                                  #C
...     pipeline.incr('trans:', -1)                     #D
...     print pipeline.execute()[0]                     #E
...
>>> if 1:
...     for i in xrange(3):                             #F
...         threading.Thread(target=trans).start()      #F
...     time.sleep(.5)                                  #G
...
1                                                       #H
1                                                       #H
1                                                       #H
#A Create a transactional pipeline
#B Queue up the 'trans:' counter increment
#C Wait for 100 milliseconds
#D Queue up the 'trans:' counter decrement
#E Execute both commands and print the result of the increment operation
#F Start three of the transactional increment/sleep/decrement calls
#G Wait half a second for everything to be done
#H Because each increment/sleep/decrement pair is executed inside a transaction, no other commands can be interleaved, which gets us a result of 1 for all of our results
#+end_src

** expiring keys

When writing data into Redis, there may be a point at which data is no longer needed.
We can remove the data explicitly with DEL, or if we want to remove an entire key after
a specified timeout, we can use what’s known as expiration. When we say that a key has
a time to live, or that it’ll expire at a given time, we mean that Redis will automatically
delete the key when its expiration time has arrived.

*Commands for handling expiration in Redis*

| Command   | Example use and description                                                                                                                                     |
|-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| PERSIST   | PERSIST key-name —Removes the expiration from a key                                                                                                             |
| TTL       | TTL key-name —Returns the amount of time remaining before a key will expire                                                                                     |
| EXPIRE    | EXPIRE key-name seconds —Sets the key to expire in the given number of seconds                                                                                  |
| EXPIREAT  | EXPIREAT key-name timestamp —Sets the expiration time as the given Unix timestamp                                                                               |
| PTTL      | PTTL key-name —Returns the number of milliseconds before the key will expire (available in Redis 2.6 and later)                                                 |
| PEXPIRE   | PEXPIRE key-name milliseconds —Sets the key to expire in the given number of milliseconds (available in Redis 2.6 and later)                                    |
| PEXPIREAT | PEXPIREAT key-name timestamp-milliseconds —Sets the expiration time to be the given Unix timestamp specified in milliseconds (available in Redis 2.6 and later) |

*** python example

#+begin_src 
>>> conn.set('key', 'value')                    #A
True                                            #A
>>> conn.get('key')                             #A
'value'                                         #A
>>> conn.expire('key', 2)                       #B
True                                            #B
>>> time.sleep(2)                               #B
>>> conn.get('key')                             #B
>>> conn.set('key', 'value2')
True
>>> conn.expire('key', 100); conn.ttl('key')    #C
True                                            #C
100                                             #C
#A We are starting with a very simple STRING value
#B If we set a key to expire in the future, and we wait long enough for the key to expire, when we try to fetch the key, it has already been deleted
#C We can also easily find out how long it will be before a key will expire
#+end_src 

** Hello world using =redis= and =Nodejs=

#+begin_src javascript
// install by: $ npm install redis
var redis = require("redis");
var client = redis.createClient();
client.set("my_key", "Hello world using nodejs and redis");
client.get("my_key",redis.print);
client.quit();
#+end_src

and run with

#+begin_src shell
$ node hello.js
#+end_src 

** Building a voting system with Strings using Node.js
   
#+begin_src javascript
var redis =require('redis');

var client = redis.createClient();

function upVote(id){
	var key = "article:" + id + ":votes";
	client.incr(key);
}

function downVote(id){
	var key = "article:" + id + ":votes";
	client.decr(key);
}

function showResults(id) {
	var headlineKey = "article:" + id + ":headline";
	var voteKey = "article:" + id + ":votes";
	client.mget([headlineKey, voteKey], function(err, replies){
		console.log("The article " + replies[0] + " has " + replies[1] + " votes");
	});
}

upVote(12345);
upVote(12345);
upVote(12345);
upVote(10001);
upVote(10001);
downVote(10001);
upVote(60056);

showResults(12345);
showResults(10001);
showResults(60056);

client.quit();
#+end_src 

** implementing a generic Queue system

=Queue.js=

#+begin_src javascript 
function Queue(queueName, redisClient){
    this.queueName = queueName;
    this.redisClient = redisClient;
    this.queueKey = 'queues: ' + queueName;
    // zero means no timeout
    this.timeout = 0;
}

Queue.prototype.size = function(callback){
    this.redisClient.llen(this.queueKey, callback);
}

Queue.prototype.push = function(data){
    this.redisClient.lpush(this.queueKey, data);
}

Queue.prototype.pop = function(callback){
    // The command BRPOP removes the last element of a Redis List.
    // If the list is emtpy , it waits until there is something  to remove.
    this.redisClient.brpop(this.queueKey, this.timeout, callback);
}

exports.Queue = Queue;

// produce/consumer implementation.

#+end_src 

=producer-worker.js=

#+begin_src javascript 
var redis= require('redis');
var client = redis.createClient();
var queue = require('./queue');
var logsQueue = new queue.Queue("logs", client);
var MAX = 5;
for(var i = 0 ; i < MAX; ++i){
    logsQueue.push("Hello world  #" + i);
}
console.log("Created " + MAX  + " logs");
client.quit();

#+end_src 

=consumer-worker.js=

#+begin_src javascript
var redis = require('redis');
var client = redis.createClient();
var queue = require("./queue");
var logsQueue = new queue.Queue("logs", client);
function logMessages(){
    logsQueue.pop(function(err, replies){
        var queueName = replies[0];
        var message = replies[1];
        console.log("[Consumer] Got log: " + message);

        logsQueue.size(function(err, size){
            console.log(size + " logs left");
        });

        logMessages();
    });
}

logMessages();

#+end_src 

run with

#+begin_src shell 
$ node consumer-worker.js
$ node producer-worker.js
#+end_src 

** A voting system with Hashes and Node.js
=hash-voting-system.js=

#+begin_src javascript
var redis = require('redis');
var client = redis.createClient();

function saveLink(id, author, title, link){
	client.hmset("Link:" + id, "author", author, "title", title, "link", link,
		score, 0);	
}

function upVote(id) {
	client.hincrby("link:" + id, "score", 1);
}

function downVote(di) {
	client.hincrby("link:" + id, "score", -1);
}

function showDetails(id) {
	client.hgetall("link: " + id, function(err, replies){
		console.log("Title: ", replies['title']);
		console.log("author: " , replies['author']);
		console.log("Link: " , replies['link']);
		console.log("score: ", replies['score']);
		console.log("------------------------------");
	})
}

saveLink(123, "dayvson", "Maxwell Dayvson's Github page", "https://github.com/dayvson");
upVote(123);
upVote(123);
saveLink(456, "hltbra", "Hugo Tavares's Github page", "https://github.com/hltbra");
upVote(456);
upVote(456);
downVote(456);
showDetails(123);
showDetails(123);

client.quit();
#+end_src 

* useful resources
* references
http://www.folkstalk.com/2012/02/cut-command-in-unix-linux-examples.html

http://unix.stackexchange.com/questions/35369/how-to-cut-by-tab-character

http://unix.stackexchange.com/questions/145978/replace-multiple-spaces-with-one-using-tr-only

http://askubuntu.com/questions/499995/change-ip-address-which-is-given-by-tor-using-the-terminal

http://superuser.com/questions/792525/how-to-change-ffmpeg-threads-settings
http://crm5.farahoosh.ir/AAA2/userinfo/118262
